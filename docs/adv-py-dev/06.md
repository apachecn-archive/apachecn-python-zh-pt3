# 六、聚集过程

既然我们已经有了从计算机收集数据并通过 HTTP 接口报告数据的健壮代码库，是时候开始记录和分析这些数据了。我们需要创建一个连接到每个传感器并提取数据的中央聚合流程。这样的过程将允许我们同时观察不同传感器之间的相关性以及随时间的趋势。

首先，我们需要创建一个新的 Python 包。对于我们来说，将聚合过程的所有代码与数据收集代码一起分发是没有意义的；我们期望部署更多的传感器，而不是聚合过程。

程序员很少从零开始一个新项目，并自己编写所有的样板文件。更常见的是使用模板，要么显式地使用，要么通过复制另一个项目并删除其功能来使用。从一段存在但什么都不做的代码开始比从一个空目录开始容易得多。

## 饼干成型切割刀

虽然您可以通过复制目录从模板创建新项目，但是有一些工具可以使这个过程变得更容易。尽管复制和修改一个模板目录看起来很简单，但它通常需要从“框架”或“示例”中重命名文件和目录，以匹配您正在创建的项目的名称。像 cookiecutter 这样的工具通过允许您创建使用首次创建项目时提供的变量的模板来自动化这个过程。

我推荐使用 cookiecutter 来创建新项目。对我们来说，这将是一个全球性的开发工具，而不是一个特定于项目的工具。我们应该将它安装到系统 Python 环境中， <sup>[1](#Fn1)</sup> ，就像我们对 Pipenv 所做的那样。

```py
> pip install --user cookiecutter

```

有许多预先存在的 cookiecutter 模板；有些为一般的 Python 包提供模板，有些为更复杂的东西提供模板。对于各种各样的东西，如混合 Python/rust 包、基于 Python 的智能手机应用和 Python web 应用，都有专门的模板。

您不需要安装 cookiecutter 模板；事实上，你不能。一个模板只能作为本地模板副本的路径或者作为 git 的远程规范被引用(比如，你通常会传递给`git clone` <sup>[2](#Fn2)</sup> )。当您指定远程模板时，cookiecutter 会自动下载并使用该模板。如果您以前已经使用过该模板，系统会提示您用新下载的版本替换它。

Tip

如果你有一个经常使用的模板，我建议你在本地保存一个。不要忘记定期更新它，以防 git 存储库中已经应用了修复，但是除了速度上的小改进，这允许您在不连接到互联网的情况下生成代码。

如果您发现自己没有网络连接，但是没有维护本地签出，那么 cookiecutter 可能在`~/.cookiecutter/`有一个来自过去调用的缓存

### 创建新模板

我们可以使用这些模板作为聚合过程的基础，但是它们都不完全符合我们在前面章节中做出的决策。相反，我将创建一个新的模板，该模板收集了本书对最小 Python 包的建议。您可以根据自己的喜好进行调整，或者创建新的模板来自动创建特定于您的工作的样板代码。

Note

如果你想使用我在这里描述的模板，你没有必要做你自己的版本。我的模板可以和`cookiecutter gh:MatthewWilkes/cookiecutter-simplepackage`一起用。本节解释了创建您自己的自定义模板的过程。

我们将创建一个新的 git 存储库来保存模板。我们需要添加的第一件事是一个`cookiecutter.json`文件，如清单 [6-1](#PC2) 所示。这个文件定义了我们将向用户询问的变量及其默认值。其中大多数都是简单的字符串，在这种情况下，会提示用户输入一个值或按 enter 键接受显示在括号中的默认值。通过将 Python 表达式括在大括号中，它们还可以包含来自早期条目的变量替换(这些条目可以是 Python 表达式)，在这种情况下，这些替换的结果将用作默认值。最后，它们可以是一个列表，在这种情况下，用户会看到一个选项列表，并被要求选择一个，第一个项目是默认的。

```py
{
    "full_name": "Advanced Python Development reader",
    "email": "example@advancedpython.dev",
    "project_name": "Example project",
    "project_slug": "{{ cookiecutter.project_name.lower().replace(' ', '_').replace('-', '_') }}",
    "project_short_description": "An example project.",
    "version": "1.0.0",
    "open_source_license": ["BSD", "GPL", "Not open source"]
}

Listing 6-1cookiecutter.json

```

我们还需要创建一个目录，其中包含我们将要创建的模板。我们还可以使用大括号在文件名中包含用户提供的值，所以这个应该被称为`{{ cookiecutter.project_slug }}`来创建一个与`project_slug`值同名的目录。我们可以使用来自`cookiecutter.json`的任何值；但是，项目 slug 是最好的选择。这个目录将成为新项目的 git 存储库的根，因此它的名称应该与预期的存储库名称相匹配。

从这里，我们可以创建我们想要包含在这种类型的每个项目中的各种文件，比如构建文件(`setup.py`、`setup.cfg`)、文档(`README.md`、`CHANGES.md`、`LICENCE`)以及`test/`和`src/`目录。

然而，有一个复杂的问题。该模板在`src/`中包含了一个`{{ cookiecutter.project_slug }}/`目录，这对于任何在其 slug 中不包含`.`的包都很好，但是如果我们正在创建`apd.sensors`，我们将会看到 cookiecutter 生成的内容与我们想要的内容之间的差异(图 [6-1](#Fig1) )。

![img/481001_1_En_6_Fig1_HTML.png](img/481001_1_En_6_Fig1_HTML.png)

图 6-1

我们拥有的文件夹结构与我们需要的文件夹结构的比较

我们需要在目录结构中增加这一层，因为`apd`是一个名称空间包。当我们第一次创建`apd.sensors`时，我们决定`apd`将是一个名称空间，这允许我们在名称空间内创建多个包，条件是没有代码直接放在名称空间包中，只有它们包含的标准包。

我们在这里需要一些自定义行为，这超出了单独使用模板所能实现的范围。 <sup>[3](#Fn3)</sup> 我们需要识别 slug 中哪里有`.`，在这种情况下，拆分 slug 并为每个部分创建嵌套目录。Cookiecutter 通过使用后一代钩子来支持这个需求。在模板的根目录中，我们可以添加一个带有`post_gen_project.py`文件的 hooks 目录。*预生成钩子*，存储为钩子/ `pre_gen_project.py`，用于在生成开始前操作和验证用户输入；*后生成钩子*，存储为钩子/ `post_gen_project.py`，用于操作生成的输出。

钩子是 Python 文件，在适当的生成阶段直接执行。它们不需要提供任何重要的功能；代码可以是模块级的。Cookiecutter 首先将这个文件解释为一个模板，在它执行钩子代码之前，任何变量都会被替换。这种行为允许使用变量将数据直接插入到钩子的代码中(如清单 [6-2](#PC3) 所示)，而不是使用更常见的 API 来检索数据。

```py
import os

package_name = "{{ cookiecutter.project_slug }}"
*namespaces, base_name = package_name.split(".")

if namespaces:
    # We need to create the namespace directories and rename the inner directory
    directory = "src"
    # Find the directory the template created: src/example.with.namespaces
    existing_inner_directory = os.path.join("src", package_name)

    # Create directories for namespaces: src/example/with/
    innermost_namespace_directory = os.path.join("src", *namespaces)
    os.mkdir(innermost_namespace_directory)

    # Rename the inner directory to the last component
    # and move it into the namespace directory
    os.rename(
        existing_inner_directory,
        os.path.join(innermost_namespace_directory, base_name)
    )

Listing 6-2hooks/post_gen_project.py

```

Note

`*namespaces, base_name = package_name.split(".")`行是扩展解包的一个例子。它与函数定义中的`*args`有相似的含义；`base_name`变量包含从`package_name`中分离出的最后一项，任何之前的项都存储为一个名为`namespaces`的列表。如果`package_name`中没有`.`字符，那么`base_name`将等于`package_name`，并且名称空间将是一个空列表。

使用我在这里创建的 cookiecutter 模板可以通过 GitHub helper 来完成，因为我已经将代码存储在 GitHub 中了。本章附带的代码中也提供了这一功能。cookiecutter 调用如下，其中`gh:`是 GitHub 助手前缀:

```py
> cookiecutter gh:MatthewWilkes/cookiecutter-simplepackage

```

或者，您可以使用您的本地工作副本来测试调用

```py
> cookiecutter ./cookiecutter-simplepackage

```

## 创建聚合包

我们现在可以使用 cookiecutter 模板为聚合过程创建一个包，名为`apd.aggregation`。切换到`apd.code`目录的父目录，但是不需要为聚合过程创建一个目录，因为我们的 cookiecutter 模板会这样做。我们调用 cookiecutter 生成器并填充我们想要的细节，然后可以用第一次提交中添加的生成文件在该目录中初始化一个新的 git 存储库。

*生成 apd.aggregation 的控制台会话*

```py
> cookiecutter gh:MatthewWilkes/cookiecutter-simplepackage
full_name [Advanced Python Development reader]: Matthew Wilkes
email [example@advancedpython.dev]: matt@advancedpython.dev
project_name [Example project]: APD Sensor aggregator
project_slug [apd_sensor_aggregator]: apd.aggregation
project_short_description [An example project.]: A programme that queries apd.sensor endpoints and aggregates their results

.
version [1.0.0]:
Select license:
1 - BSD
2 - MIT
3 - Not open source
Choose from 1, 2, 3 (1, 2, 3) [1]:
> cd apd.aggregation
> git init
Initialized empty Git repository in /apd.aggregation/.git/
> git add .
> git commit -m "Generated from skeleton"

```

下一步是开始创建实用函数和附带的测试来收集数据。作为其中的一部分，我们必须对聚合过程的确切职责以及它所提供的特性做出一些决定。

我们希望从聚合过程中获得的特性的完整列表如下。在本书的过程中，我们不一定要构建所有这些特性，但是我们需要确保我们的设计不排除其中任何一个。

*   按需从所有端点收集传感器的值

*   以特定的时间间隔自动记录传感器的值

*   调用在特定时间点为一个或多个端点记录的传感器数据

*   调用一个或多个端点在某个时间范围内的传感器数据

*   查找传感器值与某个条件(如某个范围内的最大值、最小值)相匹配的时间，可以是在所有时间内，也可以是在某个时间范围内

*   支持所有传感器类型，无需修改服务器来存储数据
    *   要求传感器安装在服务器上进行分析是可以的，但不能检索数据。

*   必须能够导出和导入兼容的数据，以实现数据可移植性和备份目的

*   必须能够按时间或端点 <sup>[4](#Fn4)</sup> 删除数据

### 数据库类型

我们需要做的第一件事是决定数据应该如何存储在这个应用中。有许多数据库可用，涵盖了各种各样的特性集。开发人员经常根据当前的流行趋势选择特定的数据库，而不是对利弊进行冷静的分析。图 [6-2](#Fig2) 是一个决策树，它概括了我在决定使用什么类型的数据库时问自己的广泛问题。这只能帮助你找到一个广泛的数据库类别，而不是一个特定的软件，因为功能集变化很大。尽管如此，我相信在决定一种类型的数据库时问这些问题是有帮助的。

![img/481001_1_En_6_Fig2_HTML.jpg](img/481001_1_En_6_Fig2_HTML.jpg)

图 6-2

挑选一类数据库的决策树

我问自己的第一个问题是排除一些数据库技术的特例。这些都是有价值的技术，在它们特定的领域，它们是优秀的，但是它们相对来说是不经常需要的。这些是只附加的数据库——一旦写入，就不能(轻易)删除或编辑。这种数据库非常适合日志，比如事务日志或审计日志。区块链数据库和仅追加数据库之间的主要区别是信任；虽然在典型情况下两者都阻止编辑或删除数据，但是可以通过操作底层存储文件来编辑标准的仅追加数据库。区块链略有不同；它允许一组人共同充当维护者。只有在至少 50%的用户同意的情况下，才能编辑或删除数据。任何不同意的用户可以保留旧数据并离开该组。在撰写本文时，区块链是当今流行的数据库，但是它们不适用于几乎所有的应用。

图左侧的数据库类型更有用。它们是 SQL 和 NoSQL 数据库。NoSQL 数据库在 21 世纪初很流行。关系数据库后来采用了它们的一些特性作为扩展和额外的数据类型。是否使用 SQL 不是区分这些数据库类型的关键方法，而是它们是否是*无模式的*。这种区别类似于有和没有类型提示的 Python 无模式数据库允许用户添加任意形状的数据，而具有已定义模式的数据库验证数据以确保它符合数据库作者的期望。无模式数据库可能看起来更有吸引力，但它会使查询或迁移数据变得更加困难。如果不能保证存在哪些列以及它们的类型，就有可能存储看起来正确的数据，但在以后的开发中会出现问题。

例如，假设我们有一个温度日志表，其中存储了记录温度值的时间、记录该温度的传感器以及该值。该值很可能被声明为十进制数，但是如果传感器提供类似于`"21.2c"`而不是`21.2`的字符串，会发生什么呢？在实施模式的数据库中，这将引发错误，数据将无法插入。在无模式数据库中，如果检索到的数据集中存在这些格式不正确的条目之一，则插入会成功，但聚合数据的尝试(如计算平均值)会失败。与 Python 的类型提示一样，这并不能防止所有的错误，只是一种类型的错误。值`70.2`将被接受，因为它是一个有效的数字，尽管人类可以分辨出它是用华氏度而不是摄氏度来度量的。

我们需要考虑的最后一件事是如何查询数据。查询支持是这三个问题中最难概括的，因为数据库类别之间有很大的差异。人们通常认为关系数据库更适合查询，而 NoSQL 数据库更依赖自然键，比如对象存储中的路径或键/值存储中的键。然而，这过于简单化了。例如，SQLite 是一个关系数据库，但是与 PostgreSQL 等替代数据库相比，它的索引选项相对较少；Elasticsearch 是一个 NoSQL 数据库，设计用于索引和搜索的灵活性。

### 我们的例子

在我们的例子中，我们发现很难决定传感器值的单一类型，除了所有值都是 JSON 可序列化的这一事实。我们希望能够访问这种类型的内部，例如，温度值的大小或 IP 地址列表的长度。如果我们要用标准的关系数据库结构来构建它，我们将很难用一种面向未来的方式来表示这些选项。我们必须预先知道可能返回的不同类型的值来编写数据库结构。

更适合我们的是使用无模式数据库，让从 API 返回的传感器的 JSON 表示成为存储的数据。我们有一个保证，我们可以准确地恢复这些数据(假设我们有相同版本的传感器代码)，并且找到一种表示它的方法没有任何困难。

这个问题把我们带到了决策树的最低决策点；我们现在需要考虑数据库中项目之间的关系。单个传感器值由于由相同的传感器类型生成、从相同的端点检索以及同时检索而与其他值相关。也就是说，传感器值通过传感器名称、端点 URL 和创建时间相关联。这些多维关系应该引导我们走向一个具有丰富索引和查询支持的数据库，因为它将帮助我们找到相关数据。我们还希望数据库具有良好的查询支持，因为我们希望能够从它们的值中找到记录，而不仅仅是传感器和时间。

这些需求引导我们使用*关系数据库* *和无模式支持*选项。也就是说，我们应该强烈地考虑这样一个数据库，它的核心是关系型的，但支持实现无模式行为的类型。PostgreSQL 及其 JSONB 类型就是一个很好的例子。JSONB 用于以 JSON 格式 <sup>[6](#Fn6)</sup> 存储数据，并允许创建在其内部结构上工作的索引。

```py
CREATE TABLE sensor_values(
    id SERIAL PRIMARY KEY,
    sensor_name TEXT NOT NULL,
    collected_at TIMESTAMP
    data JSONB
 )

```

这种格式平衡了固定模式数据库的一些优点，因为它是部分固定的。`name`和`collected_at`字段是固定列，但是剩余的数据字段是无模式字段。理论上，我们可以将 JSON 或任何其他序列化格式作为文本列存储在这个表中，但是使用 JSONB 字段允许我们编写查询和索引来检查这个值。

### 对象关系映射器

直接把 SQL 代码写成 Python 是完全可能的，但人们这样做的情况相对较少。数据库是复杂的野兽，SQL 因易受注入攻击而臭名昭著。完全抽象出单个数据库的特性是不可能的，但是确实有工具可以处理表创建、列映射和 SQL 生成。

Python 世界中最流行的是由 Michael Bayer 等人编写的 SQLAlchemy。SQLAlchemy 是一个非常灵活的对象关系映射器；它处理 SQL 语句和原生 Python 对象之间的转换，并且是以可扩展的方式完成的。另一个常用的 ORM 是 Django ORM，它不太灵活，但是提供了一个不太需要数据库工作原理的接口。一般来说，只有在 Django 项目中使用 Django ORM，否则 SQLAlchemy 是最合适的 ORM。

Note

SQLAlchemy 不附带类型提示；但是，有一个名为 *sqlmypy* 的 mypy 插件，它为 SQLAlchemy 提供提示，并教会 mypy 理解列定义所隐含的类型。我建议在使用类型检查的基于 SQLAlchemy 的项目中使用这种方法。本章附带的代码使用了这个插件。

首先，我们需要安装 SQLAlchemy 和一个数据库驱动程序。我们需要将`SQLAlchemy`和`psycopg2`添加到`setup.cfg`中的`install_requires`部分，并使用命令行上的`pipenv install -e .`触发这些依赖关系进行重新评估。

用 SQLAlchemy 描述数据库结构有两种方式，经典和声明式。在经典风格中，实例化`Table`对象并将它们与现有的类相关联。在声明式风格中，您使用一个特定的基类(它引入了一个元类)，然后您直接在面向用户的类上定义列。在大多数情况下，Python 风格的声明性方法使其成为自然的选择。

*与前面相同的表，采用 SQLAlchemy 声明风格*

```py
import sqlalchemy
from sqlalchemy.ext.declarative import declarative_base
from sqlalchemy.dialects.postgresql import JSONB, TIMESTAMP

Base = declarative_base()

class DataPoint(Base):
    __tablename__ = 'sensor_values'
    id = sqlalchemy.Column(sqlalchemy.Integer, primary_key=True)
    sensor_name = sqlalchemy.Column(sqlalchemy.String)
    collected_at = sqlalchemy.Column(TIMESTAMP)
    data = sqlalchemy.Column(JSONB)

```

然后，您可以使用 Python 代码编写查询，这会自动创建适当的 SQL。`create_engine(...)`函数用于从连接字符串创建数据库连接。设置`echo=True`可以通过，让你看到生成的 SQL。下一步是使用`sessionmaker(...)`创建一个函数，该函数允许您启动一个新的会话和事务，然后最终为数据库连接创建一个会话，如下所示:

```py
>>> engine = sqlalchemy.create_engine("postgresql+psycopg2://apd@localhost/apd", echo=True)
>>> sm = sessionmaker(engine)
>>> Session = sm()
>>> Session.query(DataPoint).filter(DataPoint.sensor_name == "temperature").all()
INFO sqlalchemy.engine.base.Engine SELECT sensor_values.id AS sensor_values_id, sensor_values.sensor_name AS sensor_values_sensor_name, sensor_values.collected_at AS sensor_values_collected_at, sensor_values.data AS sensor_values_data
FROM sensor_values
WHERE sensor_values.sensor_name = %(sensor_name_1)s
INFO sqlalchemy.engine.base.Engine {'sensor_name_1': 'temperature'}
[]

```

Column Objects And Descriptors

我们在类中使用的列对象以一种不寻常的方式运行。当我们从类中访问一列时，比如`DataPoint.sensor_name`，我们得到一个特殊的对象来表示列本身。这些对象截取许多 Python 操作，并返回表示操作的占位符。如果没有这个拦截，`DataPoint.sensor_name == "temperature"`将被求值，`filter(...)`函数将等同于`Session.query(DataPoint).filter(False).all()`。

`DataPoint.sensor_name=="temperature"`返回一个 BinaryExpression 对象。这个对象是不透明的，但是 SQL 模板(不包括常量值)可以用`str(...)`预览:

```py
>>> str((DataPoint.sensor_name=="temperature"))                                 'sensor_values.sensor_name = :sensor_name_1'

```

表达式的隐含数据库类型存储在表达式结果的`type`属性中。在比较的情况下，总是`Boolean`。

当对`DataPoint`类型的实例执行相同的表达式时，它不会保留任何特定于 SQL 的行为；该表达式正常计算对象的实际数据。SQLAlchemy 声明类的任何实例都像普通 Python 对象一样工作。

因此，开发人员可以使用相同的表达式来表示 Python 条件和 SQL 条件。

这是可能的，因为由`DataPoint.sensor_name`引用的对象是一个*描述符*。描述符是一个具有某种方法组合`__get__(self, instance, owner)`、`__set__(self, instance, value)`和`__delete__(self, instance)`的对象。

描述符允许实例属性的自定义行为，允许在类或实例上访问值时返回任意值，以及自定义设置或删除值时发生的事情。

下面是一个描述符示例，它在实例上的行为类似于普通的 Python 值，但在类上公开自己:

```py
class ExampleDescriptor:

    def __set_name__(self, instance, name):
        self.name = name

    def __get__(self, instance, owner):
        print(f"{self}.__get__({instance}, {owner})")
        if not instance:
            # We were called on the class available as `owner`
            return self
        else:
            # We were called on the instance called `instance`
            if self.name in instance.__dict__:
                return instance.__dict__[self.name]
            else:
                raise AttributeError(self.name)

    def __set__(self, instance, value):
        print(f"{self}.__set__({instance}, {value})")
        instance.__dict__[self.name] = value

    def __delete__(self, instance):
        print(f"{self}.__delete__({instance}")
        del instance.__dict__[self.name]

class A:

    foo = ExampleDescriptor()

```

下面的控制台会话演示了前面的 __get__ 方法的两个代码路径，以及设置和删除功能。

```py
>>> A.foo
<ExampleDescriptor object at 0x03A93110>.__get__(None, <class 'A'>)
<ExampleDescriptor object at 0x03A93110>
>>> instance = A()
>>> instance.foo
<ExampleDescriptor object at 0x03A93110>.__get__(<A object at 0x01664090>, <class 'A'>)
Traceback (most recent call last):
  File "<stdin>", line 1, in <module>
  File ".\exampledescriptor.py", line 16, in
    __get__raise AttributeError(self.name)
AttributeError: foo
>>> instance.foo = 1
<ExampleDescriptor object at 0x03A93110>.__set__(<A object at 0x01664090>, 1)
>>> instance.foo
<ExampleDescriptor object at 0x03A93110>.__get__(<A object at 0x01664090>, <class 'A'>)
1
>>> del instance.foo
<ExampleDescriptor object at 0x03A93110>.__delete__(<A object at 0x01664090>)

```

大多数时候，你需要一个描述符，这是为了产生一个计算结果的属性。这可以用`@property`装饰器更好地表达，它在幕后构造一个描述符。在只需要定制 get 功能的常见情况下，属性特别有用，但是它们也支持设置和删除的定制实现。

```py
class A:

    @property
    def foo(self):
        return self._foo

    @foo.setter
    def foo(self, value):
        self._foo = value

    @foo.deleter
    def foo(self):
        del self._foo

```

一些核心 Python 特性是作为描述符实现的:它们是一种非常强大的方式，可以深入到核心对象逻辑中。在不了解它们的情况下，像`@property`和`@classmethod`decorator 这样的特性看起来就像是解释器专门寻找的魔法，而不是你可以自己编程的东西。

也就是说，虽然我经常使用`@property`装饰器，但我从来没有理由编写描述符。如果您发现自己在复制/粘贴属性定义，那么您可能需要考虑将它们的代码合并到一个描述符中。

### 版本化数据库

SQLAlchemy 中有一个函数可以创建数据库中定义的所有各种表、索引和约束。这将检查已经定义的表和列，并为它们生成匹配的数据库结构。

*使用 SQLAlchemy 创建所有已定义的数据库表*

```py
engine = sqlalchemy.create_engine("postgresql+psycopg2://apd@localhost/apd", echo=True)
Base.metadata.create_all(engine)

```

这个功能一开始看起来很棒，但是非常有限。在完成一些性能测试后，您可能会在将来添加更多的表或列，或者至少添加更多的索引。`create_all(...)`函数创建了所有还不存在的东西，这意味着如果您重新运行`create_all(...)`，任何被更改但之前存在的表都不会被更新。因此，依靠`create_all(...)`可能会导致数据库包含您期望的所有表，但不包含所有列。

为了解决这个问题，人们使用 SQL 迁移框架。Alembic 是 SQLAlchemy 最受欢迎的一个。它的工作方式是连接到数据库的一个实例，并生成使连接的数据库与代码中定义的数据库同步所需的操作。如果您使用的是 Django ORM，有一个内置的迁移框架，它通过分析所有过去的迁移并将分析的状态与代码的当前状态进行比较来工作。

这些框架允许我们对数据库进行更改，并相信这些更改会传播到实际部署中，而不管他们过去使用的是什么版本的软件。如果用户跳过一个或三个版本，这些版本之间的任何迁移也将运行。

为此，我们将 Alembic 添加到`setup.cfg`依赖项列表中，然后重新运行`pipenv install -e .`来刷新这些依赖项并安装 Alembic。然后我们使用`alembic`命令行工具来生成在我们的包中使用 Alembic 所需的文件。

```py
> pipenv run alembic init src\apd\aggregation\alembic
Creating directory src\apd\aggregation\alembic ...  done
Creating directory src\apd\aggregation\alembic\versions ...  done
Generating alembic.ini ...  done
Generating src\apd\aggregation\alembic\env.py ...  done
Generating src\apd\aggregation\alembic\README ...  done
Generating src\apd\aggregation\alembic\script.py.mako ...  done
Please edit configuration/connection/logging settings in 'alembic.ini' before proceeding.

```

大多数文件都是在包内的`alembic/`目录中创建的。我们需要将文件放在这里，以便安装软件包的人可以访问它们；该层次结构之外的文件不会分发给最终用户。例外是`alembic.ini`，它提供日志和数据库连接配置。这些因最终用户而异，因此不能包含在软件包中。

我们需要修改生成的`alembic.ini`文件，主要是改变数据库 URI 以匹配我们正在使用的连接字符串。如果愿意，我们可以保留`script_location=src/apd/aggregation/alembic`的值，因为在这个开发环境中，我们使用的是`apd.aggregation`的可编辑安装，但是这个路径对于最终用户来说是无效的，所以我们应该将它改为引用一个已安装的包，并且我们应该在 readme 文件中包含一个最小的`alembic.ini`示例。

Caution

Alembic 脚本一般只适用于用户模型(依赖项有自己的配置和 ini 文件来迁移它们的模型)。用户从来没有一个有效的理由为他们的依赖关系中包含的模型生成新的迁移。另一方面，Django 的 ORM 同时处理用户模型和依赖关系，所以如果一个维护者发布了一个包的不完整版本，最终用户在生成他们自己的迁移时可能会无意中为它创建新的迁移。因此，检查迁移文件是否被正确提交和发布是非常重要的。当作为最终用户生成新的迁移时，您应该全面检查为您的代码创建的文件，而不是依赖项。

*面向最终用户的最小 alembic.ini】*

```py
[alembic]
script_location = apd.aggregation:alembic
sqlalchemy.url = postgresql+psycopg2://apd@localhost/apd

```

我们还需要在包内定制生成的代码，从`env.py`文件开始。这个文件需要一个对我们之前在使用`create_all(...)`函数时看到的元数据对象的引用，因此它可以确定代码中模型的状态。它还包含连接数据库和生成代表迁移的 SQL 文件的函数。这些可以编辑，以允许定制数据库连接选项，以满足我们的项目需求。

我们需要更改`target_metadata`行，以使用模型使用的声明性`Base`类的元数据，如下所示:

```py
from apd.aggregation.database import Base
target_metadata = Base.metadata

```

现在我们可以生成一个迁移来表示数据库的初始状态， <sup>[7](#Fn7)</sup> 这个迁移创建了我们为支持 DataPoint 类而创建的`datapoints`表。

```py
> pipenv run alembic revision --autogenerate -m "Create datapoints table"

```

修订命令在`alembic/versions/`目录中创建一个文件。名称的第一部分是随机生成的不透明标识符，但第二部分基于上面给出的消息。标志的存在意味着生成的文件不会是空的；它包含匹配代码当前状态所需的迁移操作。该文件基于一个模板，`alembic/`目录中的`script.py.mako`。这个模板是由 Alembic 自动添加的。虽然我们可以修改它，如果我们想，默认的一般是好的。改变这一点的主要原因是修改注释，也许是生成迁移时要检查的事情的清单。

在对该文件运行 black 并删除包含说明的注释后，它看起来像这样:

*alem BIC/versions/6 D2 ea CD 5 da 3f _ create _ sensor _ values _ table . py*

```py
"""Create datapoints table

Revision ID: 6d2eacd5da3f
Revises: N/A
Create Date: 2019-09-29 13:43:21.242706

"""
from alembic import op
import sqlalchemy as sa
from sqlalchemy.dialects import postgresql

# revision identifiers, used by Alembic.
revision = "6d2eacd5da3f"
down_revision = None
branch_labels = None
depends_on = None

def upgrade():
    op.create_table(
        "datapoints",
        sa.Column("id", sa.Integer(), nullable=False),
        sa.Column("sensor_name", sa.String(), nullable=True),
        sa.Column("collected_at", postgresql.TIMESTAMP(), nullable=True),
        sa.Column("data", postgresql.JSONB(astext_type=sa.Text()), nullable=True),
        sa.PrimaryKeyConstraint("id"),
    )

def downgrade():
    op.drop_table("datapoints")

```

Alembic 使用四个模块范围变量来确定迁移的运行顺序。这些不应该被改变。`upgrade()`和`downgrade()`函数的主体是我们需要检查的，以确保它们做了我们期望的所有更改，并且只做了我们期望的更改。需要进行的最常见的更改是如果检测到不正确的更改，例如迁移更改了一个列，但是目标状态等于起始状态。例如，如果数据库备份恢复不正确，就会发生这种情况。

一个不太常见(但仍然常见)的问题是，有时 alembic 迁移包括从依赖项或用户代码中的其他地方引入代码的 import 语句，通常是在开发人员使用自定义列类型时。在这种情况下，迁移必须改变，因为迁移代码完全独立是很重要的。出于同样的原因，任何常量也应该复制到迁移文件中。

如果迁移导入外部代码，那么它的效果可能会随着外部代码的改变而改变。任何影响不完全确定的迁移都可能导致现实世界的数据库具有不一致的状态，这取决于迁移时哪个版本的依赖关系代码是可用的。

Example of a Migration Repeatability Issue

例如，考虑以下用于将用户表添加到软件中的部分迁移代码:

```py
from example.database import UserStates

def upgrade():
    op.create_table(
        "user",
        sa.Column("id", sa.Integer(), nullable=False),
        sa.Column("username", sa.String(), nullable=False),
        sa.Column("status", sa.Enum(UserStates), nullable=False),
        ...
        sa.PrimaryKeyConstraint("id"),
    )

```

有一个状态字段，作为枚举字段，只能包含预选值。如果 1.0.0 版本的代码定义了`UserStates = ["valid", "deleted"]`，那么 Enum 将被创建为有效选项。然而，1.1.0 版本可能会添加另一个状态，使`UserStates = ["new", "valid", "deleted"]`表示用户在登录之前必须验证他们的帐户。1.1.0 版还需要添加一个迁移，以将“new”作为有效类型添加到该枚举中。

如果用户安装了 1.0.0 版并运行了迁移，然后安装了 1.1.0 版并重新运行了迁移，则数据库将是正确的。但是，如果用户只是在 1.1.0 发布后才了解该软件，并且在安装了 1.1.0 的情况下运行了两次迁移，那么初始迁移将添加所有三个用户状态，而第二个迁移将无法添加已经存在的值。

作为开发人员，我们习惯了不应该重复代码的想法，因为这会导致可维护性问题，但是数据库迁移是个例外。您应该复制您需要的任何代码，以确保迁移的行为不会随着时间的推移而改变。

最后，有些变化模棱两可。如果我们要更改我们在这里创建的`datapoints`表的名称，Alembic 不清楚这是名称更改还是删除一个表并创建另一个恰好具有相同结构的表。Alembic 总是在删除和重新创建方面出错，所以如果想要重命名，但迁移没有改变，就会发生数据丢失。

Alembic 文档中提供了可用操作的详细信息，它提供了您可能需要的所有日常操作。操作插件可以提供新的操作类型，尤其是特定于数据库的操作。

Tip

当您对升级操作进行更改时，也应该对降级操作进行等效的更改。如果您不想支持从特定版本降级，您应该引发一个异常，而不是保留不正确的自动生成的迁移代码。对于非破坏性迁移，允许降级非常有用，因为它允许开发人员在特性分支之间切换时恢复数据库。

随着这个迁移的生成并提交到源代码控制中，我们可以运行迁移了，它为我们生成了这个数据点表。运行迁移是通过 alembic 命令行完成的，如下所示:

```py
> alembic upgrade head

```

#### 其他有用的 alembic 命令

有一些 Alembic 用户日常需要的子命令。这些因素如下:

*   `alembic current`
    *   显示连接的数据库的版本号。

*   `alembic heads`
    *   显示迁移集中的最新版本号。如果列出了多个版本，则需要合并迁移。

*   `alembic merge heads`
    *   创建一个新的迁移，它依赖于 alembic heads 列出的所有修订，确保它们都被执行。

*   `alembic history`
    *   显示了 Alembic 已知的所有迁移的列表。

*   `alembic stamp` `<revisionid>`
    *   用字母数字版本标识符替换`<revisionid>`,将现有数据库标记为该版本，而不运行任何迁移。

*   `alembic upgrade` `<revisionid>`
    *   将`<revisionid>`替换为要升级到的字母数字版本标识符。这可以对头部 <sup>[8](#Fn8)</sup> 进行最近一次修改。Alembic 跟踪修订历史，运行尚未执行的任何迁移的升级方法。

*   `alembic downgrade` `<revisionid>`
    *   像`upgrade`，但是目标修改更早，使用降级方式。根据我的经验，这在合并迁移中不如在直接迁移中有效，您应该知道降级并不等同于撤销。它不能还原已删除的列中的数据。

### 加载数据

现在我们已经定义了数据模型，可以开始从传感器加载数据了。我们将使用优秀的*请求*库通过 HTTP 来完成这项工作。支持将 HTTP 请求内置到 Python 中，但是 requests 库有更好的用户界面。我建议在所有情况下都使用基于标准库 HTTP 支持的请求。只有在使用依赖关系不切实际的情况下，才应该使用标准库的 HTTP 请求支持。

我们从传感器中提取数据所需的最底层构建块是一个函数，该函数给定端点的 API 细节，向 API 发出 HTTP 请求，解析结果，并为每个传感器创建`DataPoint`类实例。

*从服务器添加数据点的功能*

```py
def get_data_points(server: str, api_key: t.Optional[str]) -> t.Iterable[DataPoint]:
    if not server.endswith("/"):
        server += "/"
    url = server + "v/2.0/sensors/"
    headers = {}
    if api_key:
        headers["X-API-KEY"] = api_key
    try:
        result = requests.get(url, headers=headers)
    except requests.ConnectionError as e:
        raise ValueError(f"Error connecting to {server}")
    now = datetime.datetime.now()
    if result.ok:
        for value in result.json()["sensors"]:
            yield DataPoint(
                sensor_name=value["id"], collected_at=now, data=value["value"]
            )
    else:
        raise ValueError(
            f"Error loading data from {server}: "
            + result.json().get("error", "Unknown")
        )

```

该函数连接到远程服务器，并返回每个传感器值的 DataPoint 对象。它还可以引发一个表示在试图读取数据时遇到错误的`ValueError`,并对所提供的 URL 执行一些基本的检查。

Yield and Return

我只是将`get_data_points()`函数描述为返回数据点对象的*，但这并不完全正确。它使用 yield 关键字，而不是 return。我们在第 5 章中简要地看到了这一点，当时编写了一个 WSGI 应用，它返回部分响应，中间有一个延迟。*

`yield`语句使它成为一个生成器函数。生成器是一个延迟求值的可迭代值。它可以产生零个或多个值，甚至无穷多个值。生成器只生成调用者请求的项，不像普通函数在第一个项对调用者可用之前计算完整的返回值。

构建简单生成器的最简单方法是使用生成器表达式，如果您熟悉列表、集合和字典理解，它看起来就像您想象的元组理解。

```py
>>> [item for item in range(10)]
[0, 1, 2, 3, 4, 5, 6, 7, 8, 9]
>>> (item for item in range(10))
<generator object <genexpr> at 0x01B58EB0>

```

这些生成器表达式不能像列表一样进行索引，您只能从中请求下一项:

```py
>>> a=(item for item in range(10))
>>> a[0]
Traceback (most recent call last):
  File "<stdin>", line 1, in <module>
TypeError: 'generator' object is not subscriptable
>>> next(a)
0
>>> next(a)
1
...
>>> next(a)
8
>>> next(a)
9
>>> next(a)
Traceback (most recent call last):
File "<stdin>", line 1, in <module>
StopIteration

```

也可以使用`list(a)`语法将它们转换成列表或元组(只要它们不包含无限多的条目)；然而，这考虑到了他们的状态。如果您已经从生成器中提取了一些或所有的项目，那么`list(a)`的结果将只包含那些剩余的项目。

**发电机功能**

前面的例子是生成器表达式，但是`get_data_points()`是生成器函数。它们使用`yield`关键字来指定下一个值应该是什么，然后暂停执行，直到用户请求下一个值。Python 记住了函数的状态；当请求下一项时，它从 yield 语句的点继续。

这非常有用，因为有些函数需要很长时间来生成每个后续值。另一种方法是创建一个函数，您需要指定想要生成的项目数量，但是生成器模型允许您在决定是否需要更多项目之前检查返回的项目。

考虑以下发生器函数:

```py
def generator() -> t.Iterable[int]:
    print("Stating")
    yield 1
    print("Part way")
    yield 2
    print("Done")

```

这里，`print(...)`代表更复杂的代码，可能连接到外部服务或复杂的算法。如果我们将这个生成器强制转换为一个元组，那么在我们得到结果之前，所有的打印都会发生:

```py
>>> tuple(generator())
Stating
Part way
Done
(1, 2)

```

但是，如果我们逐个使用这些项目，我们可以看到在返回的值之间执行了`yield`语句之间的代码:

```py
>>> for num in generator():
...   print(num)
...
Stating
1
Part way
2
Done

```

**何时使用它们**

有时可能不清楚最好使用生成器还是普通函数。任何只是单独生成数据的函数都可以是生成器函数或标准函数，但是对数据执行操作(比如向数据库添加数据点)的函数必须确保*消耗*迭代器。

通常所说的经验法则是，这样的函数应该返回值，而不是产生值，但是任何导致整个迭代器被求值的模式都可以。另一种方法是循环所有项目:

```py
def add_to_session(session)
    for item in generator:
        session.add(item)

```

或者通过将生成器转换成具体的列表或元组类型:

```py
def add_to_session(session)
    session.add_all(tuple(generator))

```

然而，如果在前面的函数中有一个`yield`语句，那么它们就不会像预期的那样工作。前面的两个函数都可以用`add_to_session(generator)`调用，生成器生成的所有项目都将被添加到会话中。如果以同样的方式调用以下内容，将不会向会话中添加任何项目:

```py
def add_to_session(session)
    for item in generator:
        session.add(item)
        yield item

```

如果有疑问，请使用标准函数，而不是生成器函数。无论哪种方式，请确保您测试了您的函数的行为是否符合预期。

Exercise 6-1: Practice With Generators

编写一个生成器函数，从单个传感器提供无限量的数据点。您应该在您构建的`DataPoint`实例上使用`yield`，并在使用`time.sleep(...)`函数的采样之间等待一秒钟。

一旦编写了这个函数，就应该循环遍历它的值，以查看传感器被查询时数据的突发情况。您还应该尝试使用标准库的`filter(function, iterable)`函数来查找特定传感器的值。

本章附带的代码中提供了一个实现示例。

这个函数是一个很好的开始:它提供了我们可以迭代的包含`DataPoint`对象的东西，但是我们需要创建一个数据库连接，将它们添加到一个会话中，然后提交那个会话。为此，我定义了两个助手函数(如清单 [6-3](#PC31) 所示)，一个函数给定一个数据库会话和服务器信息，从每个服务器获取所有数据点，并调用`session.add(point)`将它们添加到当前数据库事务中。第二个是作为一个独立的数据收集功能。它建立会话，调用`add_data_from_sensors(...)`，然后将会话提交给数据库。我还创建了另一个基于单击的命令行工具来执行这些操作，允许在命令行上传递参数。

```py
def add_data_from_sensors(
    session: Session, servers: t.Tuple[str], api_key: t.Optional[str]
) -> t.Iterable[DataPoint]:
    points: t.List[DataPoint] = []
    for server in servers:
        for point in get_data_points(server, api_key):
            session.add(point)
            points.append(point)
    return points

def standalone(
    db_uri: str, servers: t.Tuple[str], api_key: t.Optional[str], echo: bool = False
) -> None:
    engine = sqlalchemy.create_engine(db_uri, echo=echo)
    sm = sessionmaker(engine)
    Session = sm()
    add_data_from_sensors(Session, servers, api_key)
    Session.commit()

Listing 6-3Helper functions in collect.py

```

*点击 cli.py* 中的 input point

```py
@click.command()
@click.argument("server", nargs=-1)
@click.option(
    "--db",
    metavar="<CONNECTION_STRING>",
    default="postgresql+psycopg2://localhost/apd",
    help="The connection string to a PostgreSQL database",
    envvar="APD_DB_URI",
)
@click.option("--api-key", metavar="<KEY>", envvar="APD_API_KEY")
@click.option(
    "--tolerate-failures",
    "-f",
    help="If provided, failure to retrieve some sensors' data will not " "abort the collection process",
    is_flag=True,
)
@click.option("-v", "--verbose", is_flag=True, help="Enables verbose mode")
def collect_sensor_data(
    db: str, server: t.Tuple[str], api_key: str, tolerate_failures: bool, verbose: bool
):
    """This loads data from one or more sensors into the specified database.

    Only PostgreSQL databases are supported, as the column definitions use
    multiple pg specific features. The database must already exist and be
    populated with the required tables.

    The --api-key option is used to specify the access token for the sensors
    being queried.

    You may specify any number of servers, the variable should be the full URL
    to the sensor's HTTP interface, not including the /v/2.0 portion. Multiple
    URLs should be separated with a space.
    """
    if tolerate_failures:
        attempts = [(s,) for s in server]
    else:
        attempts = [server]
    success = True
    for attempt in attempts:
        try:
            standalone(db, attempt, api_key, echo=verbose)
        except ValueError as e:
            click.secho(str(e), err=True, fg="red")
            success = False
    return success

```

这个示例使用了 click 的更多特性，包括 click 命令上的文档字符串作为命令的帮助向最终用户公开。帮助文本大大增加了函数的长度，但是在带有语法突出显示的代码编辑器中，帮助文本就不那么冗长了。这在用户使用`--help`标志时公开，如下所示:

```py
> pipenv run collect_sensor_data --help
Usage: collect_sensor_data [OPTIONS] [SERVER]...

  This loads data from one or more sensors into the specified database

.

  Only PostgreSQL databases are supported, as the column definitions use
  multiple pg specific features. The database must already exist and be
  populated with the required tables.

  The --api-key option is used to specify the access token for the sensors
  being queried.

  You may specify any number of servers, the variable should be the full URL
  to the sensor's HTTP interface, not including the /v/2.0 portion. Multiple
  URLs should be separated with a space.

Options:
  --db <CONNECTION_STRING>  The connection string to a PostgreSQL database
  --api-key <KEY>
  -f, --tolerate-failures   If provided, failure to retrieve some sensors'
                            data will not abort the collection process

  -v, --verbose             Enables verbose mode
  --help                    Show this message and exit.

```

然后，我们第一次使用`@click.argument`。我们用它来收集函数的参数，而不是带有相关值的选项。这个参数的`nargs=-1`选项声明我们接受任意数量的参数，而不是特定的数量(通常是`1`)。因此，该命令可以作为`collect_sensor_data` `http://localhost:8000/`(仅从本地主机收集数据)、作为`collect_sensor_data` `http://one:8000/` `http://two:8000/`(从两个服务器收集数据)，甚至作为`collect_sensor_data`(不收集数据，但会隐式测试数据库连接)来调用。

`--api-key`和`--verbose`选项可能不需要任何解释，但是`--tolerate-failures`选项是我们可能没有考虑到的。如果没有这个选项及其支持代码，我们将对所有传感器位置运行`standalone(...)`函数，但是如果其中一个失败，整个脚本都会失败。此选项允许用户指定在指定了多个服务器的情况下，任何成功的服务器都会保存其数据，而失败的传感器会被忽略。代码通过使用该选项来决定是否应该从`[("` `http://one:8000/` `", "` `http://two:8000/` `")]`或`[("` `http://one:8000/` `", ), ("` `http://two:8000/` `", )]`下载数据来实现这一点。这个命令的代码在正常情况下将所有服务器传递给`standalone(...)`，但是如果添加了`--tolerate-failures`，那么对于每个服务器 URL，将有一个对`standalone(...)`的调用。这是一个非常方便的特性，但是如果我自己使用这个命令的话，我会喜欢这个特性。

最后，支持功能相对简单。`add_data_from_sensors(...)`函数包装现有的`get_data_points(...)`函数，并在其返回的每个数据点上调用`session.add(...)`。然后它将这些作为返回值传递给调用者，但作为一个列表而不是一个生成器。当我们遍历生成器时，它确保了迭代器被完全消耗。对`add_data_from_sensors(...)`的调用可以访问`DataPoint`对象，但是它们没有义务遍历这些对象来使用生成器。

Caution

喜欢函数式编码风格的开发人员有时会陷入一个陷阱。他们可能想用类似于`map(Session.add, items)`的东西来代替这个功能。map 函数创建了一个生成器，因此需要消耗它才能产生效果。这样做可能会引入一些微妙的错误，比如只有在启用了 verbose 标志的情况下代码才起作用，这会导致 iterable 被日志记录语句消耗掉。

Do not use

`map(...)` **如果你对项目调用的函数有任何副作用，比如用数据库会话注册对象。总是使用循环来代替；它更加清晰，并且没有对后续代码施加任何义务来确保生成器被消耗掉。**

## 新技术

我们已经略微谈到了一些非常常用的技术。我建议花点时间去理解我们在这一章中对它们的使用所做的所有决定。为此，我在下文中简要概述了我的建议。

### 数据库

选择一个与你需要处理的数据相匹配的数据库，而不是当前流行的数据库。有些数据库，比如 PostgreSQL，是很好的默认选择，因为它们提供了很大的灵活性，但是灵活性是以复杂性为代价的。

如果您使用基于 SQL 的数据库，请使用 ORM 和迁移框架。除了极端的情况，它们比编写自己的定制 SQL 更好地为您服务。但是，不要误以为 ORM 会保护你不了解数据库。它简化了界面，但是如果您试图在不了解数据库需求的情况下与数据库进行交互，您将会遇到困难。

### 自定义属性行为

如果您需要一个行为类似于计算属性的东西，也就是说，一个行为类似于对象上的属性但实际上从其他来源构建其值的东西，`@property`是最好的方法。对于一次性的值包装器来说也是如此，其中数据被修改或重新格式化。在这种情况下，应该使用带有 setter 的属性。

如果您正在编写一个要在代码库中多次使用的行为(尤其是如果您正在构建一个供他人使用的框架)，描述符通常是一个更好的选择。您可以用属性做的任何事情都可以用自定义描述符来完成，但是您应该更喜欢属性，因为它们看起来更容易理解。如果你创建了一个行为，你应该小心确保它不会偏离其他开发者从 Python 代码中期望的行为太远。

### 发电机

生成器适用于希望提供无限(或非常长)的值流进行循环的情况。如果生成器的用户不需要保存所有先前值的记录，则可以使用它们来减少内存消耗。这种优势也可能是它们最大的缺点:除非消耗掉整个生成器，否则不能保证生成器函数中的代码能够执行。

不要使用生成器，除非是在需要生成一个只读取一次的项目列表的函数中，在生成过程预计会很慢的情况下，以及在您不确定调用者是否需要处理所有项目的情况下。

## 摘要

在这一章中我们已经做了很多:我们已经创建了一个新的包，引入了 ORM 和迁移框架，并且在幕后窥视了一些 Python 解释器用来确定当你访问一个对象的属性时会发生什么的深层魔法。我们还有一个有效的聚合流程，可以从每个传感器中提取数据并存储起来以备后用。

在下一章中，当我们看到如何在 Python 中实现异步编程以及何时异步编程是解决问题的合适方法时，我们将更深入地研究`yield`功能的复杂用法。

### 额外资源

我建议查看以下资源，以了解更多关于我们在本章中使用的技术。像往常一样，请随意阅读那些你感兴趣的或者与你的工作相关的内容。

*   Julia Evans 的*成为精选明星*(已付费，样品可在 [`https://wizardzines.com/zines/sql/`](https://wizardzines.com/zines/sql/) 获得)是一个以可打印格式对关系数据库细节的迷人解释。如果您是关系数据库的新手，这是一个很好的起点。

*   关于 JSON 类型的 PostgreSQL 文档详细介绍了从 JSON 字段中提取信息的查询行为。那是在 [`www.postgresql.org/docs/current/datatype-json.html`](http://www.postgresql.org/docs/current/datatype-json.html) 。

*   blogpost [`https://www.citusdata.com/blog/2016/07/14/choosing-nosql-hstore-json-jsonb/`](http://www.citusdata.com/blog/2016/07/14/choosing-nosql-hstore-json-jsonb/) 提供了一些关于如何在 PostgreSQL 中的两种 JSON 列变体之间进行选择的好技巧，以及我在本书中没有涉及的更老的`hstore`类型。

*   关于描述符的 Python 文档中有许多描述符如何被用来实现标准库特性的例子: [`https://docs.python.org/3/howto/descriptor.html`](https://docs.python.org/3/howto/descriptor.html) 。

*   在 [`http://cookiecutter-templates.sebastianruml.name/`](http://cookiecutter-templates.sebastianruml.name/) 有一个 Cookiecutter 模板的聚合器。

<aside aria-label="Footnotes" class="FootnoteSection" epub:type="footnotes">Footnotes [1](#Fn1_source)

Cookiecutter 有相当多的依赖项。到目前为止，在我们安装的所有系统工具中，这是我最想隔离的一个。您可以使用 pipenv 为 cookiecutter 创建一个环境，并将与该环境相关联的`bin/`(或者，在 Windows 上，`Scripts/`)目录添加到您的系统路径中(运行`pipenv --venv`来找到它)。如果您的系统 Python 环境是一个非常旧的版本，您可能也需要这样做。

  [2](#Fn2_source)

还有流行的 Git 托管平台的帮助者，比如 GitHub。例如，`gh:MatthewWilkes/cookiecutter-simplepackage`将引用我的 GitHub 帐户上的`cookiecutter-simplepackage`存储库。

  [3](#Fn3_source)

仅仅使用一个模板*是有可能实现这一点的，但前提是该模板特定于所使用的嵌套名称空间包的数量。*

  [4](#Fn4_source)

导出和删除选项对于传感器与公众共处一地的任何部署都特别重要，例如 2004 年在阿姆斯特丹周围的家庭中使用噪声传感器来监控飞机噪声。我们以尊重用户和公众隐私的方式开发软件，这一点很重要。

  [5](#Fn5_source)

当人们谈论数据的形状时，他们指的是数据类型的结构。例如，`{“foo”: 2}`与`{“bar”: 99}`形状相同，但与`[“foo”, 2]`和{ `”foo”: “2”}`不同。

  [6](#Fn6_source)

JSON 有两种格式，`JSON`和`JSONB`。`JSONB`在加载数据时解析 JSON，但是`JSON`稍微宽容一些。如果您需要存储包含重复键、有意义的空白或有意义的键排序的 JSON，您应该使用`JSON`类型而不是`JSONB`。如果您不打算在 JSON 数据中搜索，那么使用`JSONB`的开销可能不值得。

  [7](#Fn7_source)

我们可以用本章开头提到的`Base.metadata.create_all(engine)`命令生成初始状态，但这只是因为当前状态也是初始状态。如果我们做了任何改变，那么`create_all(...)`将不再产生初始状态。将它放在初始迁移中意味着用户总是可以通过升级到数据库的最新版本来设置数据库。

  [8](#Fn8_source)

使用`heads`也将升级到最新版本，但是它将遵循任何分支路径，而不是要求它们被合并。我建议不要使用这个功能，而是确保合并任何分叉的迁移。

 </aside>