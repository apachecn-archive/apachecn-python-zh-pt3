# 五、网络机器人

任何在网上呆过一段时间的人都会告诉你，互联网上有很多信息。根据谷歌的索引，截至 2013 年，共有 40.4 亿个网页，所以今天的网页数量可能会比这多得多。当然，这些网页中有很多可能是猫的图片和色情内容，但也有数以亿计的网页上有信息。*有用的*信息。据说每一条被数字化的信息都存在于互联网的某个地方。它必须被发现——当互联网看起来有点像图 [5-1](#Fig1) 时，这不是一件容易的事情。

![img/323064_2_En_5_Fig1_HTML.jpg](img/323064_2_En_5_Fig1_HTML.jpg)

图 5-1

互联网地图(2013 [`http://internet-map.net`](http://internet-map.net) ，鲁斯兰·埃尼科夫)

不幸的是，任何人都不可能下载并阅读他或她感兴趣的所有信息。人类就是没那么快，我们不得不吃饭、睡觉、完成各种低效的、有时令人不快的任务，比如洗澡和工作来谋生。

幸运的是，我们可以给计算机编程，让它做一些我们自己不需要做的枯燥、重复的任务。这是网络机器人的功能之一:我们可以给机器人编程，让它抓取网页，跟踪链接，下载文件。它通常被称为机器人，知道如何编程和使用机器人是一项非常有用的技能。早上醒来时需要股票报告吗？让你的机器人抓取国际索引，并有一个电子表格等着你。需要研究白星航空公司发布在网上的所有乘客名单，寻找你的祖先吗？让你的机器人从谷歌的“白星”开始，遍历所有的链接。或者你可能想找到所有埃德加·爱伦·坡的手稿，这些手稿目前都在公共领域；当你睡觉的时候，一个机器人也可以帮忙。

Python 特别适合做网络机器人的工作，在这种情况下也被称为*蜘蛛*。有几个模块需要下载，然后你可以编写一个全功能的机器人来执行你的命令，从你给它的任何页面开始。因为遍历网页和下载信息并不是一个非常耗费处理器资源的任务，所以它也非常适合 Raspberry Pi。当你的普通台式计算机处理更困难的计算任务时，Pi 可以处理下载网页、解析文本和跟随链接下载文件所需的少量工作。

## Bot 标签

如果你要建立一个有效的网络爬虫，你需要记住的一个因素是*机器人礼仪*。我不是指在喝下午茶时确保机器人的小指伸展的礼仪。相反，当你设计你的机器人抓取网站时，你应该注意一些细节。

一个是尊重`robots.txt`文件。大多数网站在网站的根目录中都有这个文件。这是一个简单的文本文件，包含访问机器人和蜘蛛的指令。如果网站的所有者不希望某些页面被抓取和索引，他可以在文本文件中列出这些页面和目录，礼貌的机器人会同意他的请求。文件格式很简单。看起来是这样的:

```py
User-agent: *
Disallow: /examples/
Disallow: /private.html

```

这个`robots.txt`文件指定任何机器人(`User-agent: *`)都不能访问/ `examples/`文件夹中的任何页面，也不能访问页面`private.html`。`robots.txt`文件是一种标准机制，网站可以通过它来限制对某些页面的访问。如果你想让你的机器人在所有网站都受欢迎，遵循这些规则是个好主意。我将解释如何去做。如果你选择忽略这些规则，你可以经常期待你的机器人(和所有来自你的 IP 地址的访问)被禁止(和阻止)从有问题的网站。

另一项礼节是控制你的机器人信息请求的速度。因为机器人是计算机，它们访问和下载页面和文件的速度比人类快成百上千倍。出于这个原因，一个机器人完全有可能在如此短的时间内向一个站点发出如此多的请求，以至于它可以使一个配置很差的 web 服务器瘫痪。因此，将您的机器人的页面请求保持在可管理的水平是礼貌的；大多数网站所有者对每秒 10 个页面请求没什么意见——这远远超过了手工处理的速度，但不足以让服务器停机。同样，在 Python 中，这可以通过一个简单的`sleep()`函数来完成。

最后，伪造您的*用户代理身份*经常会有问题。用户代理身份标识网站的访问者。Firefox 浏览器有一个特定的用户代理，Internet Explorer 有另一个，而机器人又有另一个。因为有许多网站根本不希望机器人访问或抓取他们的页面，所以一些机器人作者给他们的机器人一个欺诈用户代理，使它看起来像一个正常的网络浏览器。这一点都不酷。你可能永远不会被发现，但这是一个基本的礼仪问题——如果你有你想保密的页面，你也会希望别人尊重你的意愿。为其他网站所有者做同样的事情。这只是成为一个好的机器人作家和网民(网民)的一部分。如果您出于其他目的模拟浏览器，例如站点测试或查找和下载文件(pdf、MP3 等)，但不是为了对这些站点进行爬网，则可以模拟浏览器的用户代理。

## 网络的连接

在我们开始编程我们的蜘蛛之前，你需要了解一点互联网是如何运作的。是的，它基本上是一个巨大的计算机网络，但这个网络遵循一定的规则，使用一定的协议，我们需要利用这些协议在网络上做任何事情，包括使用蜘蛛。

### 网络通信协议

超文本传输协议(HTTP)是封装大多数常见 web 流量的格式。协议只是两个通信方(在这种情况下是计算机)之间关于如何进行通信的协议。它包括的信息有:数据如何寻址、如何确定传输过程中是否发生了错误(以及如何处理这些错误)、信息如何在源和目的地之间传输以及信息如何格式化。大多数 URL(统一资源定位符)前面的“http”定义了用于请求页面的协议。其他常用的协议有 TCP/IP(传输控制协议/互联网协议)、UDP(用户数据报协议)、SMTP(简单邮件传输协议)和 FTP(文件传输协议)。使用哪种协议取决于多种因素，如流量类型、请求速度、数据流是否需要按顺序提供服务，以及这些数据流的容错能力。

当你用浏览器请求一个网页时，幕后会发生一些好事。假设你在地址栏中输入 [`http://www.irrelevantcheetah.com`](http://www.irrelevantcheetah.com) 。您的电脑知道它正在使用 HTTP 协议，首先向其本地 DNS(域名系统)服务器发送 [`www.irrelevantcheetah.com`](http://www.irrelevantcheetah.com) ，以确定它属于哪个互联网地址。DNS 服务器用一个 IP 地址来响应——比如说，192.185.21.158。这是保存该域网页的服务器的地址。域名系统将 IP 地址映射到名称，因为你我记住“ [`www.irrelevantcheetah.com`](http://www.irrelevantcheetah.com) ”比记住“192.185.21.158”要容易得多

既然您的电脑知道了服务器的 IP 地址，它就会使用三次“握手”来启动与该服务器的 TCP 连接服务器响应，你的计算机请求页面`index.html`。服务器响应，然后关闭 TCP 连接。

然后，您的浏览器读取页面上的代码并显示出来。如果它需要页面的其他部分，比如 PHP 代码或图像，那么它会向服务器请求这些部分或图像，并显示它们。

### 网页格式

大多数网页都是 HTML 格式的——超文本标记语言。它是 XML(可扩展标记语言)的一种形式，非常容易阅读和解析，并且可以被大多数计算机理解。浏览器被编程来解释页面的语言，并以某种方式显示这些页面。例如，标签对`<html>`和`</html>`表示页面是 HTML 格式的。`<i>`和`</i>`表示被包围的文本为*斜体*，而`<a>`和`</a>`表示一个 [*超链接*](http://en.wikipedia.org/wiki/Hyperlink) ，通常显示为蓝色并带有下划线。JavaScript 被`<script type="text/javascript"></script>`标签包围着，各种其他更复杂的标签包围着各种语言和脚本。

所有这些标签和格式使得浏览和阅读原始网页变得容易。然而，它们也有一个令人愉快的副作用，那就是让计算机很容易解析这些页面。毕竟，如果你的浏览器不能解码网页，互联网就不会以现在的形式存在。但是你不需要浏览器来请求和阅读网页——只需要在你得到网页后显示它们。您可以编写一个脚本来请求网页，阅读它们，并使用网页信息执行预先编写好的任务——所有这些都无需人工干预。因此，您可以自动执行搜索特定链接、页面和格式化文档的漫长而枯燥的过程，并将其传递给您的 Pi。这就是网络机器人。

### 请求示例

为了简单起见，我们先说我们已经请求了页面 [`http://www.carbon111.com/links.html`](http://www.carbon111.com/links.html) 。该页面的文本非常简单——毕竟，它是一个静态页面，没有花哨的 web 表单或动态内容，看起来很像这样:

```py
<HTML>
<HEAD>
<TITLE>Links.html</TITLE>
</HEAD>
<BODY BACKGROUND="mainback.jpg" BGCOLOR="#000000"
 TEXT="#E2DBF5" LINK="#EE6000" VLINK="#BD7603" ALINK="#FFFAF0">
<br>
<H1 ALIGN="CENTER">My Favorite Sites and Resources</H1>
<br>
<H2>Comix, Art Gallerys and Points of Interest:</H2>
<DL>
<DT><A HREF="http://www.alessonislearned.com/index.html" TARGET="blank">
A Lesson Is Learned...</A>
<DD>Simply amazing! Great ideas, great execution. I love the depth of humanity
these two dig into. Not for the faint-of-heart ;)
.
.
.

```

依此类推，直到最后结束`<HTML>`标记。

如果蜘蛛通过 TCP 连接接收这个页面，它首先会知道这个页面是 HTML 格式的。然后，它将学习页面标题，并开始查找它要查找的内容(如. mp3 或。pdf 文件)以及到其他页面的链接，这些将包含在`<A></A>`标签中。蜘蛛也可以被编程为跟随链接到某个“深度”；换句话说，您可以指定机器人是否应该跟随链接页面的链接，或者是否应该在第二层之后停止跟随链接。这是一个重要的问题，因为如果你编程了太多的层，你的蜘蛛可能最终会搜索(并下载)整个互联网——如果你的带宽和存储空间有限，这将是一个严重的问题！

## 我们的网络机器人概念

我们的 web bot 背后的概念如下:我们将基于用户输入从某个页面开始。然后，我们将确定我们要查找什么文件—例如，我们要查找什么。公共领域作品的 pdf 文件？我们喜欢的乐队的免费 MP3 怎么样？这个选择也会被编入我们的机器人。

然后，机器人将从起始页面开始，解析页面上的所有文本。它将查找包含在`<a href></a>`标签(超链接)中的文本。如果超链接以“.”结尾。pdf”或“. mp3”或其他选择的文件类型，我们将调用 *wget* (一个命令行下载工具)将文件下载到我们的本地目录。如果我们找不到任何链接到我们选择的文件类型，我们将开始跟随我们*做*找到的链接，按照我们预先确定的递归方式重复这些链接的过程。当我们想走多远就走多远时，我们应该有一个目录，里面装满了我们闲暇时可以细读的文件。这就是网络机器人的用途——让电脑完成繁忙的工作，而你却在啜饮玛格丽塔酒，等待享受它的劳动成果。

## 解析网页

*解析*指的是计算机“读取”网页时所经历的过程。在最基本的情况下，网页只不过是由比特和字节(一个字节是八个比特)组成的数据流，当被解码时，形成数字、字母和符号。一个好的解析程序不仅可以将数据流重新格式化为正确的符号，还可以读取重新格式化的数据流并“理解”它所读取的内容。web bot 需要能够解析它加载的页面，因为这些页面可能/应该包含指向它被编程来检索的信息的链接。Python 提供了几种不同的文本解析器模块，我鼓励您进行试验，但是我发现最有用的是 Beautiful Soup。

### 注意

《美丽的汤》以刘易斯·卡罗尔(1855 年)的《素甲鱼之歌》命名:

美丽的汤，如此丰富和绿色

在热腾腾的盖碗里等着！

谁会为了这样的美味不弯腰？

今晚的汤，好喝的汤！

今晚的汤，好喝的汤！

美汤(Python 库)经历了几个版本；在撰写本文时，它的版本是 4.6.0，可以在 Python 2.x 和 3.x 中运行。

Beautiful Soup 的语法非常基本。一旦您通过键入以下命令安装了它

```py
sudo apt-get install python-bs4

```

在您的终端中，您可以开始在您的脚本中使用它。通过键入`python`打开 Python 提示符，并尝试键入以下内容:

```py
import BeautifulSoup

```

如果你得到一个错误信息说`No module named BeautifulSoup`，你可能正在使用一个老版本的 Beautiful Soup 4。在这种情况下，请键入

```py
from bs4 import BeautifulSoup

```

然后，继续使用 Python 提示符:

```py
import re
doc = ['<html><head><title>Page title</title></head>',
    '<body><p id="firstpara" align="center">This is paragraph <b>one</b>.',
    '<p id="secondpara" align="blah">This is paragraph <b>two</b>.',
    '</html>']
soup = BeautifulSoup(".join(doc)) #That's two apostrophes, one after another, not a double quote

```

这将加载名为`doc`的文件，该文件包含一个网页流的样子——一个长的单字符流。然后，Soup 将这些行加载到一个可以被库解析的文件中。如果您在此时键入`print soup`，它看起来将与键入`print doc`的结果相同。

然而，如果你输入

```py
print soup.prettify()

```

你将得到这个页面，以一种可读性更好的方式重新制作。这只是一个美丽的汤可以做什么的例子；当我们开始编写机器人程序时，我会详细介绍它。

顺便说一下:在前面的例子中导入的`re`模块用于计算文本中的正则表达式。如果您不熟悉正则表达式，那么它是一种极其通用的方法，可以在文本中进行搜索，并以人类读者可能不会立即看到的方式挑选出字符串和字符序列。正则表达式术语可能看起来像完全的胡言乱语；正则表达式的一个很好的例子是序列`(?<=-)\w+`，它在连字符后面的字符串中搜索字符序列。要尝试一下，通过键入`python`打开 Python 提示符，然后键入

```py
import re
m = re.search('(?<=-)\w+', 'free-bird')
m.group(0)

```

你会得到奖励

```py
bird

```

虽然正则表达式在查找文本和字符串中的字符序列方面非常有帮助，但它们也不是非常直观，并且远远超出了本书的范围。我们不会在这里花太多时间。你知道它们的存在就够了，如果你对它们感兴趣，你可以花些时间去了解它们。

## 使用 Python 模块编码

当谈到使用不同的 Python 模块来编写网络蜘蛛时，您有相当多的选择。许多开源蜘蛛已经存在，你可以从它们那里借鉴，但是从头开始编写蜘蛛代码是一个很好的学习经历。

我们的蜘蛛需要做几件事来做我们需要它做的事情。它需要

*   启动 TCP 连接和请求页面；

*   解析接收到的页面；

*   下载它找到的重要文件；和

*   跟随它遇到的链接。

幸运的是，其中大多数都是非常简单的任务，所以对我们的蜘蛛编程应该相对简单。

### 使用机械化模块

可能是自动化网页浏览最常用的模块， *mechanize* 既简单又复杂。它使用简单，只需几行代码就可以设置好，但是它也包含了许多用户没有充分利用的特性。对于网站测试等自动化任务来说，它是一个非常棒的工具:如果你需要用 50 种不同的用户名/密码组合登录一个站点 50 次，然后填写一个地址表单，mechanize 是你的首选工具。它的另一个好处是它在幕后做了大量的工作，例如启动 TCP 连接和与 web 服务器协商，这样您就可以专注于下载部分。

要在脚本中使用 mechanize，您必须首先下载并安装它。如果您一直跟着做，您仍然可以打开 Python 提示符，但是您需要一个常规的命令行界面来完成这个下载和安装过程。这里，您有两个选择:您可以退出 Python 入口模式，或者您可以打开另一个终端会话。如果您希望只打开一个终端会话，可以通过键入`Ctrl+d`退出当前窗口中的 Python 提示符，这将使您返回到正常的终端提示符。另一方面，如果您选择打开另一个终端会话，您可以让 Python 会话继续运行，到目前为止您输入的所有内容都将保留在内存中。

无论选择哪个选项，在命令行提示符下输入

[T2`https://pypi.python.org/packages/source/m/mechanize/mechanize-0.3.6.tar.gz`](https://pypi.python.org/packages/source/m/mechanize/mechanize-0.3.6.tar.gz)

下载完成后，用

```py
tar -xzvf mechanize-0.3.6.tar.gz

```

并通过键入以下命令导航到结果文件夹

```py
cd mechanize-0.3.6.tar.gz

```

然后，输入

```py
sudo python setup.py install

```

按照任何屏幕上的指示，mechanize 将被安装并准备使用。

### 用美汤解析

我前面提到过解析；美丽的汤仍然是最好的选择。如果您还没有这样做，请输入

```py
sudo apt-get install python-bs4

```

让包管理器完成它的工作。之后就可以立即使用了。正如我之前所说的，一旦你下载了这个页面，Beautiful Soup 就负责找到链接，并把它们传递给我们用来下载的函数，同时也负责把后面要用到的那些链接放在一边。

然而，这样做的结果是，查找链接和决定下载什么的工作主要变成了字符串的问题。换句话说，链接(以及其中包含的文本)只不过是字符串，在我们解开这些链接并跟踪它们或下载它们的过程中，我们将对字符串做大量的工作——从`lstrip`(删除最左边的字符)到`append`到`split`以及字符串库中的各种其他方法。毕竟，也许网络机器人最有趣的部分不是它下载的文件；更确切地说，这是你必须要做的操作。

### 使用`urllib`库下载

这个难题的最后一部分是`urllib`库——特别是它的`URLopener.retrieve()`函数。这个功能是用来下载文件的，流畅不忙乱。我们将把我们的文件名传递给它，让它去做它的工作。

要使用`urllib`，必须先导入。使用 Python 提示符切换到终端，如果它仍然打开的话，或者通过键入`python`启动另一个会话。然后，键入

```py
import urllib

```

以使其可供使用。

`urllib`库使用以下语法:

```py
image = urllib.URLopener()
image.retrieve ("http://www.website.com/imageFile.jpg", "imageFile.jpg")

```

其中发送给`URLopener.retrieve()`函数的第一个参数是文件的 URL，第二个参数是文件将要保存的本地文件名。第二，文件名参数遵循 Linux 文件和目录约定；如果你给它参数"`../../imageFile.jpg`"，`imageFile.jpg`将被保存在目录树中的两个文件夹中。同样，向它传递参数“`pics/imageFile.jpg`”会将它保存在当前目录(脚本运行的目录)内的`pics`文件夹中。但是，该文件夹必须已经存在；`retrieve()`不会创建目录。这是一件需要记住的重要事情，因为它也会无声无息地失败；也就是说，如果该目录不存在，您的脚本将会像一切正常一样执行，然后第二天早上您会发现您下载的这两千条记录中没有一条被保存到磁盘上。

## 决定下载什么

这可能会有点棘手，因为*和*太多了。不幸的是(或者幸运的是，取决于你的观点)，很多都是有版权的，所以即使你发现它是免费的，只是下载它真的不酷。你要找的东西就在外面。

然而，这是另一本完全不同的书的主题。目前，让我们假设你要寻找免费的信息，比如马克·吐温在公共领域的所有作品。这意味着你可能要寻找。pdf，。txt，甚至可能。医生或。docx 文件。你甚至可能想扩大你的搜索参数包括。mobi (Kindle)和。epub 文件，以及. chm. (chm 代表编译的 HTML，微软在他们的 HtMl 格式的帮助程序中使用它，它也经常用于基于 web 版本的教科书。)这些都是合法的文件格式，可能包含您正在寻找的书籍的文本。

### 选择起点

接下来你需要的是一个起点。你可能倾向于说“谷歌！“但是，对于一个简单的搜索“马克·吐温”会有数以千万计的搜索结果，你可能会更专注一点。提前做一些基础工作，这样可以节省你(和你的机器人)的工作时间。例如，如果你能找到吐温作品的在线档案，那将是一个很好的起点。如果你正在寻找免费的音乐下载，你可能想得到一个收录了崭露头角的乐队的新音乐文件的博客列表，因为许多新艺术家在这些博客上提供免费的歌曲下载，以推广他们自己和他们的音乐。同样，关于 IEEE 网络规范的技术文档可能在一个技术网站上找到，甚至是一个政府网站，比广泛的谷歌搜索更成功(和更集中的结果)。

### 存储您的文件

您可能还需要一个地方来存储您的文件，这取决于您的 Pi 的 SD 卡的大小。该卡既充当 RAM，也是文件存储的地方，所以如果你使用 32GB 的卡，你会有很多空间。pdf 文件。然而，如果你正在下载免费的纪录片文件，8GB 的卡可能会很快就满了。因此，你需要一个外置 USB 硬盘，要么是一个完整的硬盘，要么是一个较小的闪存驱动器。

同样，这也是一些实验可能派上用场的地方，因为一些外部驱动器不能很好地与 Raspberry Pi 一起工作。因为现在它们不是特别贵，我会买一两个中等大小的试一试。我目前正在使用戴恩-ELEC 的 8GB 闪存驱动器(如图 [5-2](#Fig2) 所示)，没有任何问题。

![img/323064_2_En_5_Fig2_HTML.jpg](img/323064_2_En_5_Fig2_HTML.jpg)

图 5-2

用于存储文件的通用闪存驱动器

关于通过命令行访问你的跳转驱动器的注意事项:在`/media`目录中可以访问连接的驱动器，例如闪存驱动器；也就是说，

```py
cd /media

```

将带您到您应该看到您的驱动器列表的目录。然后，您可以导航到其中并访问其内容。您需要设置您的 Python 脚本来将文件保存到那个目录中，例如`/media/PENDRIVE`或`/media/EnglebertHumperdinckLoveSongs`。也许最简单的方法是将你的`webbot.py`脚本保存在你的外部驱动器上，然后从那里运行它。

## 编写 Python 机器人

让我们开始写一些 Python。下面的代码导入了必要的模块，并使用 Python 版本的 *input* ( `raw_input`)来获得一个起点(在这个起点上，我已经添加了在每个网址中都可以找到的`http://`)。然后它用`mechanize.Browser()`启动一个“浏览器”(带引号)。本章末尾列出了最终完成的代码。它也可以作为`webbot.py`从`apress.com`网站下载。

要开始编写您的机器人，请使用您的文本编辑器创建一个名为`webbot.py`的新文件。输入以下内容:

```py
from bs4 import BeautifulSoup
import mechanize
import time
import urllib
import string

start = "http://" + raw_input ("Where would you like to start searching?\n")
br = mechanize.Browser()
r = br.open(start)
html = r.read()

```

稍后，我们可能需要伪造一个用户代理，这取决于我们访问的站点，但是这段代码现在可以用了。

### 读取字符串并提取所有链接

一旦你有了一个 browser 对象，在前面的代码中叫做`br`，你就可以用它来做各种各样的任务。我们使用`br.open()`打开用户请求的起始页，并将其读入一个长字符串`html`。现在，我们可以使用 Beautiful Soup 来读取该字符串，并通过添加以下行来从中提取所有链接:

```py
soup = BeautifulSoup(html)
for link in soup.find_all('a'):
    print (link.get('href'))

```

现在，运行脚本来进行试验。保存并关闭它。打开一个终端会话，导航到您创建`webbot.py`的目录。然后打字

```py
python webbot.py

```

启动程序，并在程序询问从哪里开始时键入`example.com`。它应该返回以下内容，然后退出:

[T2`http://www.iana.org/domains/example`](http://www.iana.org/domains/example)

您已经成功地阅读了 [`http://example.com`](http://example.com) 的内容，提取了链接(只有一个)，并将该链接打印到屏幕上。这是一个令人敬畏的开始。

下一个合乎逻辑的步骤是实例化一个链接列表，每当 Beautiful Soup 找到另一个链接时就添加到这个列表中。然后可以遍历列表，用另一个浏览器对象打开每个链接，重复这个过程。

### 查找和下载文件

然而，在实例化这个链接列表之前，我们还需要创建一个函数——实际查找和下载文件的函数！因此，让我们在页面上搜索文件类型的代码。我们可能应该回过头来，通过在脚本的开头、start 行之后添加下面的代码行，来询问我们正在寻找什么类型的文件:

```py
filetype = raw_input("What file type are you looking for?\n")

```

### 注意

如果您想知道，在这两种情况下，`raw_input`字符串末尾的`\n`都是回车符。显示该行时，它不会被打印出来。相反，它会将光标移动到下一行的开头，等待您的输入。这是不必要的—它只是让输出看起来更漂亮一点。

现在我们知道我们在寻找什么，当我们把每个链接添加到列表中时，我们可以检查它是否是一个我们想要的文件的链接。如果我们要找。例如，pdf 文件，我们可以解析链接，看看它是否以 pdf 结尾。如果是，我们将调用`URLopener.retrieve()`并下载文件。因此，再次打开您的`webbot.py`副本，并用以下代码替换`for`代码块:

```py
for link in soup.find_all('a'):
    linkText = str(link)
    if filetype in linkText:
        # Download file code here

```

您会注意到这段代码中的两个元素。首先，添加了`str(link)`位。Beautiful Soup 为我们找到了页面上的每个链接，但是它将链接作为一个链接对象返回，这对于非 Soup 代码来说是没有意义的。我们需要将它转换成一个字符串，以便使用它并进行我们所有的巧妙操作。这就是调用`str()`方法的作用。事实上，Beautiful Soup 为我们提供了一种方法，但是学习用`str()`函数解析字符串是很重要的。事实上，这就是我们在代码开头使用行`import string`的原因——这样我们就可以与字符串对象交互。

第二，一旦链接是一个字符串，你可以看到我们如何使用 Python 的`in`调用。类似于 C#的`String.contains()`方法，Python 的`in`调用只是搜索字符串，看它是否包含请求的子串。在我们的案例中如果我们要找的是。pdf 文件，我们可以在链接文本中搜索子字符串“pdf”如果有，那就是我们感兴趣的链接。

### 测试机器人

为了让测试我们的机器人更容易，我在 [`http://www.irrelevantcheetah.com/browserimages.html`](http://www.irrelevantcheetah.com/browserimages.html) 设置了一个页面用于测试。它包含图像、文件、链接和各种其他 HTML 好东西。使用这个页面，我们可以从一些简单的东西开始，比如图像。因此，让我们修改我们的`webbot.py`代码，使它看起来像这样:

```py
import mechanize
import time
from bs4 import BeautifulSoup
import string
import urllib
start = "http://www.irrelevantcheetah.com/browserimages.html"
filetype = raw_input ("What file type are you looking for?\n")
br = mechanize.Browser()
r = br.open(start)
html = r.read()
soup = BeautifulSoup(html)

for link in soup.find_all('a'):
    linkText = str(link)
    fileName = str(link.get('href'))
    if filetype in fileName:
        image = urllib.URLopener()
        linkGet = http://www.irrelevantcheetah.com + fileName
        filesave = string.lstrip(fileName, '/')
        image.retrieve(linkGet, filesave)

```

我认为，从`for`循环开始的这最后一段代码需要一些解释。`for`循环遍历 Beautiful Soup 为我们找到的所有链接。然后，`linkText`将这些链接转换成字符串，这样我们就可以操作它们了。然后我们将链接的*主体*(链接指向的实际文件或页面)也转换成一个字符串，并检查它是否包含我们正在寻找的文件类型。如果是的话，我们把它附加到站点的基本 URL，给我们`linkGet`。

由于`retrieve()`函数，最后两行必须发生。正如您所记得的，该函数有两个参数:我们正在下载的文件的 URL 和我们想要保存该文件的本地名称。`filesave`获取我们之前找到的`fileName`，并从名称中删除前面的`/`，以便我们可以保存它。如果我们不这样做，我们试图保存的`fileName`将会是——例如—img/flower1.jpg`。如果我们试图用那个名字保存一个图像，Linux 会尝试将`flower.jpg`保存到`/images`文件夹，然后给我们一个错误，因为`/images`文件夹不存在。通过去掉前面的“`/`”,`fileName`变成了`images/flower1.jpg`，只要我们当前的目录中有一个`images`文件夹(记住我说的先创建目录)，文件就会被保存，不会发生任何事件。最后，最后一行代码使用我已经提到的两个参数进行实际下载:`linkGet`和`filesave`。

如果您在当前目录中创建一个`images`目录，然后运行这个脚本，对文件类型问题回答“jpg”,`images`目录应该会充满 12 个不同的美丽花朵的图像，这些图像是您真正亲手挑选的。简单，嗯？相反，如果你创建一个`files`目录并回答“pdf”，你会在你的`files`文件夹中得到 12 个不同的(无聊的)pdf。

### 创建目录和实例化列表

我们还需要添加两个特性来完成这个机器人。首先，我们并不总是能够提前知道需要创建什么目录，所以我们需要找到一种方法，从链接文本中解析文件夹名称，并动态地创建目录。其次，我们需要创建一个链接到其他页面的链接列表，这样我们就可以访问这些页面并重复下载过程。如果我们这样做几次，我们就有了一个真正的网络机器人，跟随链接并下载我们想要的文件。

让我们首先执行第二项任务——实例化我们前面提到的链接列表。我们可以在脚本的开头、导入语句之后创建一个列表，并在执行过程中添加到列表中。要创建一个列表，我们只需使用

```py
linkList = []

```

为此，我们在脚本中添加了一个`elif`块:

```py
if filetype in fileName:
    image = urllib.URLopener()
    linkGet = http://www.irrelevantcheetah.com + fileName
    filesave = string.lstrip(fileName, '/')
    image.retrieve(linkGet, filesave)

elif "htm" in fileName: # This covers both ".htm" and ".html" filenames
    linkList.append(link)

```

就这样！如果`fileName`包含我们正在寻找的链接类型，它将被检索。如果没有，但其中有一个`htm`，它会被追加到`linkList`——一个列表，然后我们可以一个接一个地迭代，打开每个页面并重复下载过程。

我们将多次重复下载过程的事实应该让你想到编码的一个元素:一个*函数*——也称为*方法*。请记住，如果有一个过程需要你一遍又一遍地重复，那么在代码中就会用到函数。它使代码更干净、更简单，也更容易编写。你会发现，程序员是非常高效的人(有人会说他们懒惰)。如果我们可以编写一次代码并重用它，这比一遍又一遍地输入要好得多。这也是一个巨大的时间节省。

因此，让我们通过在我们刚刚添加的`linkList = []`行之后，向我们的`webbot.py`脚本添加以下行来开始我们的下载功能:

```py
def downloadFiles (html, base, filetype, filelist):
    soup = BeautifulSoup(html)
    for link in soup.find_all('a'):
        linkText = str(link.get('href'))
        if filetype in linkText:
            image = urllib.URLopener()
            linkGet = base + linkText
            filesave = string.lstrip(linkText, '/')
            image.retrieve(linkGet, filesave)
        elif "htm" in linkText:  # Covers both "html" and "htm"
            linkList.append(link)

```

现在我们有了我们的`downloadFiles`函数，我们所要做的就是解析我们的`linkText`来获得我们需要创建的目录的名称。

同样，这是简单的字符串操作，并使用了`os`模块。`os`模块允许我们操作目录和文件，不管我们运行的是什么操作系统。首先，我们可以添加

```py
import os

```

添加到我们的脚本中，然后我们可以通过添加

```py
os.makedirs()

```

您可能还记得，为了简化文件保存，我们需要在机器上有一个本地目录，该目录与存储目标文件的 web 目录相匹配。为了查看我们是否需要一个本地目录，我们需要首先确定该目录的名称。在大多数(如果不是全部)情况下，该目录将是我们的`linkText`的第一部分；例如 img/picture1.html`中的目录名是`images`。所以，第一步是再次迭代`linkText`，寻找斜线，就像我们获得网站名称的基础一样，就像这样:

```py
slashList = [i for i, ind in enumerate(linkText) if ind == '/']
directoryName = linkText[(slashList[0] + 1) : slashList[1]]

```

前面的代码创建了一个索引列表，在这个列表中可以找到字符串`linkText`中的斜杠。然后，`directoryName`将`linkText`切割到前两个斜线之间的部分 img/picture1.html`被切割到`images`)。

这段代码的第一行有一些解释，因为它是重要的一行代码。`linkText`是一个字符串，因此*是可枚举的*；也就是说，其中的字符可以一个接一个地迭代。`slashList`是斜线所在的`linkText`中的位置(索引)列表。在第一行填充了`slashList`之后，`directoryName`简单地抓取包含在第一个和第二个斜杠之间的文本。

接下来的两行只是检查是否存在匹配`directoryName`的目录；如果没有，我们就创建它:

```py
if not os.path.exists(directoryName):
    os.makedirs(directoryName)

```

这就完成了我们的`downloadProcess`功能，同时也完成了我们简单的网络机器人。试一试，把它指向 [`http://www.irrelevantcheetah.com/browserimages.html`](http://www.irrelevantcheetah.com/browserimages.html) ，询问 jpg、pdf 或 txt 文件类型，然后看它创建文件夹和下载文件——所有这些都不需要你的帮助。

既然你有了这个想法，你就可以为之疯狂了！创建目录，冲浪三个(或更多)层次深，看看你的机器人为你下载，而你不看！一半的乐趣是有时在你最意想不到的时候看到什么被下载了！

## 最终代码

在这里，你可以看到你一点一点输入的最后的冗长的代码，如果你一直跟着我们学习这一章的话。同样，如果你不想全部输入，它可以在 Apress.com 上以`webbot.py`的名称下载。然而，我*高度*推荐你键入代码，因为如果你键入代码，比简单地复制粘贴会更有效。我的一位教授曾经说过，把代码输入进去，你就拥有了自己的代码。

```py
import mechanize
import time
from bs4 import BeautifulSoup
import re
import urllib
import string
import os

def downloadProcess(html, base, filetype, linkList):
    "This does the actual file downloading"
    Soup = BeautifulSoup(html)
    For link in soup.find('a'):
        linkText = str(link.get('href'))
        if filetype in linkText:
            slashList = [i for i, ind in enumerate(linkText) if ind == '/']
            directoryName = linkText[(slashList[0] + 1) : slashList[1]]
            if not os.path.exists(directoryName):
                os.makedirs(directoryName)

            image = urllib.URLopener()
            linkGet = base + linkText
            filesave = string.lstrip(linkText, "/")
            image.retrieve(linkGet, filesave)
        elif "htm" in linkText:  # Covers both "html" and "htm"
            linkList.append(link)

start = "http://" + raw_input ("Where would you like to start searching?\n")
filetype = raw_input ("What file type are you looking for?\n")

numSlash = start.count('/') #number of slashes in start—need to remove everything after third slash
slashList = [i for i, ind in enumerate(start) if ind == '/'] #list of indices of slashes

if (len(slashList) >= 3): #if there are 3 or more slashes, cut after 3
    third = slashList[2]
    base = start[:third] #base is everything up to third slash
else:
    base = start

br = mechanize.Browser()
r = br.open(start)
html = r.read()
linkList = [] #empty list of links

print "Parsing" + start
downloadProcess(html, base, filetype, linkList)

for leftover in linkList:
    time.sleep(0.1) #wait 0.1 seconds to avoid overloading server
    linkText = str(leftover.get('href'))
    print "Parsing" + base + linkText
    br = mechanize.Browser()
    r = br.open(base + linkText)
    html = r.read()
    linkList = []
    downloadProcess(html, base, filetype, linkList)

```

## 摘要

在这一章中，你通过编写一个网络机器人(或蜘蛛)很好地了解了 Python，它可以为你遍历互联网并下载你感兴趣的文件，甚至可能在你睡觉的时候。您使用了一两个函数，构造并添加了一个列表对象，甚至做了一些简单的字符串操作。

在下一章，我们将离开数字世界，与一种非常物理的现象——天气——互动。