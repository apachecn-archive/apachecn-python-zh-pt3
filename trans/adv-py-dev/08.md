# 八、高级异步

既然我们已经决定 asyncio 是一种适用于我们的聚合流程的技术，我们需要确保我们工作的代码是生产质量的。到目前为止，我们已经忽略了 apd.aggregation 代码库中的任何测试；是时候解决这个问题了，还有我们在前一章中顺便提到的阻塞数据库集成的问题。

## 测试异步代码

我们可以使用现有的工具来测试我们的异步代码，但是我们需要做一些小的调整来设置异步环境。一种方法是修改单个测试函数，通过包装函数调用`asyncio.run(...)`。这确保了测试系统是完全同步的，但是对于每一个单独的测试，都会建立一个事件循环，调度一个协同程序，并阻塞执行，直到它完成。

我们可以通过编写一个包含任何异步安装和拆卸的异步函数来实现这一点；然后，任何同步设置、拆卸和断言都被添加到主测试函数中。

```py
def test_get_data_points_fails_with_bad_api_key(self, http_server):
    async def wrapped():
        async with aiohttp.ClientSession() as http:
            return await collect.get_data_points(http_server, "incorrect", http)

    with pytest.raises(
        ValueError,
        match=f"Error loading data from {http_server}: Supply API key in " f"X-API-Key header",
    ):
        asyncio.run(wrapped())

```

前面的例子使用了一个`http_server` fixture，它将 URL 返回给一个 API 服务器，然后创建一个协程来建立一个 aiohttp 会话并调用测试中的方法`get_data_points(...)`。这里在清晰度方面有很大的牺牲:代码是无序的。首先列出异步代码，接着是断言，然后是同步代码。通常，我们根据程序的流程更自由地混合代码和断言。尽管我们可以将一些断言工作转移到测试的异步部分，但是总会有额外的代码为内部函数设置异步环境。

另一种方法是使用 pytest 插件来自动处理包装。这样做，使得混合标准测试方法和测试协程成为可能。任何使用 pytest 标记系统标记为 asyncio 测试的协程都是在异步环境中执行的，所有包装工作都在插件中透明地进行。

使用插件允许更清晰的执行流，不需要任何样板代码来弥合同步和异步代码之间的差距，如下所示:

```py
@pytest.mark.asyncio
async def test_get_data_points_fails_with_bad_api_key(self, http_server):
    with pytest.raises(
        ValueError,
        match=f"Error loading data from {http_server}: Supply API key " f"in X-API-Key header",
    ):
        async with aiohttp.ClientSession() as http:
            await collect.get_data_points(http_server, "incorrect", http)

```

Caution

我们在这里引入了一个依赖项，尽管它只在运行测试时适用。我们没有在`setup.cfg`中列出测试依赖项，只是选择将它们作为开发依赖项包含在 Pipfile 中。因此，我们可以用

`pipenv install --dev pytest-asyncio`

这在大多数情况下是没问题的，但是在较大的代码库中，您可能需要测试组件和版本的组合，而不是只有一个 Pipfile。可以在`setup.cfg`中列出测试依赖关系，以避免重复。为此，创建一个名为“test”的新的`[options.extras_require]`行，并在那里列出测试依赖项。有一个遗留的 setuptools 特性叫做 tests_require，你可能有时会看到，但是我总是推荐一个额外的，因为它提供了对是否安装测试依赖的更明确的控制。

### 测试我们的代码

编写异步测试函数的能力是一个很好的开始，但是我们还需要设置一些装置来提供聚合代码传感器端点进行询问。对此有两种方法:我们可以提供模拟数据作为聚合测试的一部分，或者让聚合测试依赖于服务器代码并启动一个真实的(尽管是临时的)服务器。

这两种选择都不是特别有吸引力的前景；它们都有明显的缺点。如果我们编写测试来检查一个已知的 HTTP 响应，那么每次底层 API 改变时都需要更新这个响应。希望这不会经常发生，但是当人们阅读测试代码时，不透明的 JSON 块很难推理。

通常，操作大量数据的测试是通过复制输入数据，运行测试，然后使用输出数据编写一个`assert`语句来编写的。这是一种有点危险的做法，因为它测试的是确保什么都没有改变，而不是检查某件事情是否正确。

另一种方法是运行后端服务器并连接到后端服务器，这是一种更现实的方法，可以避免在测试中使用原始 JSON，但是这增加了测试对服务器代码的依赖性。因此，所有的测试都需要创建一个套接字连接，并且增加了服务器安装和拆卸的开销。

这个困境和我们在第 5 章中面临的问题是一样的，我们必须在测试命令行界面的输出和直接测试传感器的功能之间做出选择。一旦我们认识到这一点，决定做什么就容易多了。功能测试为检查事情是否按预期运行提供了广泛的基础，但是更快、更专业的测试更容易开发。至关重要的是，两者都有助于我们区分底层平台发生变化时的测试失败和更快的测试对真实行为建模不佳时的测试失败。

因此，我将添加相同的标记来将这些测试声明为功能测试。在第 [5](05.html) 章中，我们在单个测试方法上用`@pytest.mark.functional`做了这些，还有一个定义了功能标记的`pytest.ini`文件。因为我们对这个包的所有功能测试都在一个不包含任何非功能测试的模块中，所以我们可以标记整个模块。通过设置`pytestmark`模块变量来引用标记，类或模块可以有一个标记，如下所示:

```py
import pytest

pytestmark = [pytest.mark.functional]

```

#### 拆除测试服务器和 pytest 装置

对于我们的测试设置，我们需要做的第一件事是实例化一个测试服务器。服务器需要提供 HTTP 套接字，因为我们正在测试发出 HTTP 请求的代码。我们需要一个监听我们可以指定的端口的服务器，这样我们可以避免与其他软件的端口冲突；我们可能需要多台服务器同时运行，以测试数据是否可以从多个端点聚合。

在我们最初的`apd.sensors`包中，我们创建了一个`set_up_config(...)`函数，它接受配置值和一个可选的`app`参数，然后将这些配置变量应用到应用程序中。如果没有提供`app`，那么使用默认的应用程序(在已知的 URL 上设置各种 API 版本)。

为了创建具有不同配置的多个 flask 应用程序，我们需要能够创建功能上等同于默认应用程序的 flask 应用程序，这对于我们的测试来说意味着它们必须在`/v/2.0`上提供 v2.0 API。我们可以通过复制来自`apd.sensors`的一些代码来创建一个新的`get_independent_flask_app(...)`函数，如清单 [8-1](#PC4) 所示。

```py
from concurrent.futures import ThreadPoolExecutor
import typing as t
import wsgiref.simple_server

import flask
import pytest

from apd.sensors.wsgi import v20
from apd.sensors.wsgi import set_up_config

def get_independent_flask_app(name: str) -> flask.Flask:
    """ Create a new flask app with the v20 API blueprint loaded, so multiple copies
    of the app can be run in parallel without conflicting configuration """
    app = flask.Flask(name)
    app.register_blueprint(v20.version, url_prefix="/v/2.0")
    return app

def run_server_in_thread(name: str, config: t.Dict[str, t.Any], port: int) -> t.Iterator[str]:
    # Create a new flask app and load in required code, to prevent config # conflicts
    app = get_independent_flask_app(name)
    flask_app = set_up_config(config, app)
    server = wsgiref.simple_server.make_server("localhost", port, flask_app)

    with ThreadPoolExecutor() as pool:
        pool.submit(server.serve_forever)
        yield f"http://localhost:{port}/"
        server.shutdown()

@pytest.fixture(scope="session")
def http_server() -> t.Iterator[str]:
    yield from run_server_in_thread(
        "standard", {"APD_SENSORS_API_KEY": "testing"}, 12081
    )

Listing 8-1Helper functions and a fixture to run a HTTP server

```

这个函数允许我们创建具有独立配置的 flask 应用程序，但所有应用程序都在正确的 URL 上包含 v2.0 API。`run_server_in_thread(...)`实用函数是一个更高级的函数，用于创建 flask 应用程序，对其进行配置，并使其服务于请求。

Note

对于是否值得向测试方法中添加类型定义，还存在一些争议。我发现 PyTest 对类型支持的缺乏移除了大部分的实用程序，但是它在很大程度上依赖于您的代码库。如果你对类型有很好的了解，你会发现这是值得的。我个人推荐类型检查实用函数，在测试方法和 fixtures 中添加返回类型注释。这通常足以确保您的测试助手在使用时进行类型检查，但是我建议在测试方法的类型方面更加务实，我经常跳过这一点。

为了服务请求，我们将使用标准库中的 wsgiref 服务器。我们之前使用它的`serve_forever()`函数来处理请求，作为测试`apd.sensors` HTTP 服务器的一部分。这几乎正是我们想要的，因为它采用了一个 WSGI 应用程序，并通过 HTTP 使它可用；但是它是以阻塞的方式实现的。一旦我们调用`serve_forever()`，服务器正常运行，直到用户用`<CTRL+c>`中断它。这不是我们想要的测试设备，所以我们需要卸载它来并发运行。

线程化的执行模型非常适合这一点:我们可以产生一个新的线程来处理`serve_forever()`调用，并在我们处理完服务器后中断它。与我们以前编写的 fixtures 不同，我们不只是想创建一个值并将其传递给测试方法，我们还想进行设置、传递一个值，然后进行拆卸以清理我们已经创建的线程。

进行设置和拆卸的 Pytest fixtures 使用关键字`yield`而不是`return`，有效地使 fixture 成为一个单项生成器。在`yield`关键字之前的任何东西都被正常执行，产生的值是作为参数提供给测试函数的。`yield`之后的任何操作仅在夹具拆除后执行。默认情况下，夹具在每次测试结束时都会被拆除。我们可以将范围更改为“`session`”，这意味着每次 pytest 调用时，fixture 应该只设置和拆除一次，而不是在每次测试之后。

这种结构允许在最后一个需要`http_server`的测试完成后进行`server.shutdown()`调用和线程池清理。

Note

shutdown 方法是标准库中 WSGIServer 的一个实现细节，但它是一个关键的细节。一旦我们的测试方法执行完毕，我们想要关闭服务请求的线程。如果我们不这样做，那么测试程序将挂起，等待线程完成，但是线程在正常操作中永远不会终止。shutdown 方法操作一个内部标志，wsgiref 服务器每 500 毫秒检查一次该标志。如果它被设置，`serve_forever()`调用返回，因此导致线程退出。

线程中运行的任何东西都必须在进程完成之前被明确关闭。在这种情况下，我们很幸运这个 API 在设计时就考虑到了这一点，但是如果你使用的是其他不提供关闭功能的 API，你可能需要创建自己的共享变量，并在提交给池的函数中检查它。不可能从外部强制线程停止；你的线程必须在不再需要的时候停止。

utility 函数允许我们创建多个这样的测试服务器，仅在配置上有所不同，并将它们的地址传递给测试方法。我们可以创建尽可能多的装置，向每个装置传递不同的数据。例如，下面给出了一个设置服务器的 fixture，该服务器使用不同的 API 键，因此会拒绝请求:

```py
@pytest.fixture(scope="session")
def bad_api_key_http_server():
    yield from run_server_in_thread(
        "alternate", {"APD_SENSORS_API_KEY": "penny"}, 12082
    )

```

这里最后要提到的是夹具本身的`yield from`结构。一个`yield` `from`表达式在构建发电机时非常有用。当给定一个 iterable 时，它放弃值，然后将执行传递给下一行。这允许编写遵从另一个迭代器的迭代器，作为更复杂的实现的一部分，例如，在现有迭代器的开头和结尾附加附加项的迭代器。它还可以用来将多个迭代器链接在一起，尽管标准库中的`itertools.chain`函数可能更适合这个目的。<sup>2[2](#Fn2)T7】</sup>

```py
def additional(base_iterator):
    yield "Start"
    yield from base_iterator
    yield "End"

```

Pytest 对待 fixture 的值与对待 fixture 的值是不同的，所以尽管我们不想操作我们正在包装的迭代器，但是我们需要对它进行迭代并产生单个值，以便 pytest 知道这个 fixture 有设置和拆卸。Pytest 通过内省 fixture 函数并检查它是否是一个生成器函数来确定这一点。 <sup>[3](#Fn3)</sup> 如果包装函数体是`return run_server_in_thread(...)`，那么，尽管调用函数的实际结果是一样的，但函数本身不会被认为是生成器函数。这是一个返回生成器的函数。

自省函数允许 fixtures 有意返回生成器，比如下面的例子返回一个只有一个值的生成器。如果这个 fixture 被用在一个测试函数中，那么这个函数将被赋予生成器本身，而不是它的单个值。

```py
@pytest.fixture
def single_item_iterator():
    def gen_func():
        yield "An item"
    return gen_func()

```

#### `Fixture scoping`

默认情况下，所有的 fixture 都在测试级别，这意味着 fixture 代码对于依赖它们的每个测试都运行一次。我们创建一个新的 HTTP 服务器的 fixtures 的作用域是在会话级别，这意味着它们只运行一次，并且所有测试共享这个值。

夹具可以使用其他夹具，作为在多个夹具和测试之间共享设置代码的一种方式。例如，在未来，作为`apd.sensors`的服务器设置的一部分，我们可能需要更多的配置值。在这种情况下，我们不想为每个正在设置的 HTTP 服务器都重复它们；我们希望将默认配置放在一个夹具中，如清单 [8-2](#PC8) 所示。这样，HTTP 服务器设备和任何需要配置值的测试都可以读取它。

```py
import copy

@pytest.fixture(scope="session")
def config_defaults():
    return {
        "APD_SENSORS_API_KEY": "testing",
        "APD_SOME_VALUE": "example",
        "APD_OTHER_THING": "off"
    }

@pytest.fixture(scope="session")
def http_server(config_defaults) -> t.Iterator[str]:
    config = copy.copy(config_defaults)
    yield from run_server_in_thread("standard", config, 12081)

@pytest.fixture(scope="session")
def bad_api_key_http_server(config_defaults) -> t.Iterator[str]:
    config = copy.copy(config_defaults)
    config["APD_SENSORS_API_KEY"] = "penny"
    yield from run_server_in_thread(
        "alternate", config, 12082
    )

Listing 8-2Changes to the fixtures to support a common config fixture

```

这个假设的`config_defaults` fixture 已经设置了`scope="session"`,因为它也在会话范围级别运行。然而，这是由会话范围的 fixtures 使用的逻辑结果，而不是自由选择。如果`config_defaults`夹具的范围更窄，那么就会出现矛盾。应该根据狭窄的范围设置和拆除它，还是在拆除依赖于它的会话范围的项目之后设置和拆除它？

我们的例子可能看起来无害，但是如果 fixture 返回动态值，或者设置一些资源，那么行为需要一致。因此，任何试图使用范围比正在使用它的 fixture 更窄的 fixture 的操作都会导致 pytest 失败，并出现范围不匹配错误，如下所示:

```py
ScopeMismatch: You tried to access the 'function' scoped fixture 'config_defaults' with a 'session' scoped request object, involved factories
tests\test_http_get.py:57:  def http_server(config_defaults)
tests\test_http_get.py:49:  def config_defaults()

```

开发人员可以使用几个作用域；这些是(从最窄到最宽)`function`、`class`、`module`、`package`、<sup>、 [4](#Fn4) 、</sup>、`session`。缺省值是 function，任何定义了显式作用域的 fixture 必须只依赖于使用该作用域或更宽作用域的 fixture。例如，任何类范围的 fixture 都可以依赖于类、模块、包或会话 fixture，但不能依赖于函数范围的 fixture。

有点令人困惑的是，还有第二种类型的范围适用于 fixtures，它们的*可发现性*。这由代码库中定义 fixture 的位置来定义。它决定了哪些函数可以使用 fixture，但是对如何在测试之间共享 fixture 调用没有影响。

我们之前创建的 HTTP 服务器设备被指定为在*会话*范围内，但是它们被定义在一个测试模块中，这使得它们的可发现性等同于*模块*范围。有三种可能的可发现性范围，相当于类、模块和包。在`conftest.py`模块中定义的夹具可用于代码库中的所有测试；在一个测试模块中定义的可用于该模块中的所有测试；而那些被定义为测试类的方法的测试对该类中的所有测试都是可用的。

发现范围与定义范围不同是很常见的，特别是当 fixture 的默认范围是 function 时，它没有等价的可发现性范围。如果可发现性比声明的范围更广，那么在整个测试过程中，可以多次设置、使用和拆卸夹具。如果是相同的，那么夹具将被设置、使用，然后立即拆除。最后，如果一个测试声明的范围比它的可发现性更广，那么它将不会被拆除，直到测试运行中的某个稍后的点，可能是在不再需要它之后很久。表 [8-1](#Tab1) 展示了这三种可能性。

表 8-1

15 种不同范围组合的效果

<colgroup><col class="tcol1 align-left"> <col class="tcol2 align-left"> <col class="tcol3 align-left"> <col class="tcol4 align-left"> <col class="tcol5 align-left"> <col class="tcol6 align-left"></colgroup> 
|   | 

`scope=function`

 | 

`scope=class`

 | 

`scope=module`

 | 

`scope=package`

 | 

`scope=session`

 |
| --- | --- | --- | --- | --- | --- |
| **定义在一个类中** | 多次调用 | 一次祈祷 | 延迟拆卸 | 延迟拆卸 | 延迟拆卸 |
| **在模块**中定义 | 多次调用 | 多次调用 | 一次祈祷 | 延迟拆卸 | 延迟拆卸 |
| **在 conftest.py** 中定义 | 多次调用 | 多次调用 | 多次调用 | 一次祈祷 | 延迟拆卸 |

如果存在多个同名的装置，那么每个测试使用发现范围最窄的一个。也就是说，在`conftest.py`中定义的 fixture 可用于所有的测试，但是如果一个模块有一个同名的 fixture，那么这个 fixture 将用于模块内的测试。如果一个类有一个同名的 fixture，情况也是如此。

Caution

这种超越仅仅是关于发现；对夹具的寿命及其拆卸行为没有影响。如果您有一个设置和拆除资源的 fixture，比如我们的 HTTP 服务器，并且您为一个类覆盖了它，那么同一 fixture 的其他版本可能已经设置好了，但是还没有拆除。 <sup>[5](#Fn5)</sup> 任何时候你定义一个 fixture，其中使用的最窄覆盖和使用的最宽声明范围在表 [8-1](#Tab1) 中被列为“延迟拆卸”，你*必须*确保你的 fixture 不试图持有相同的资源，例如 TCP/IP 套接字。

我们的代码中确实有不匹配的地方:我们的 HTTP 服务器 fixture 是在一个测试模块中定义的，但是使用了会话范围，所以它可能会遭受延迟拆卸。我们可以通过将 fixtures 移动到`conftest.py`或者将声明的范围更改为`module`来解决这个问题。我们需要决定我们是否希望我们的 fixture 与测试运行保持一致，并且可供任何测试使用，或者我们是否希望它只供`test_http_get.py`测试模块使用，并且一旦这些测试被执行，它就被拆除。

由于我们不打算创建一个需要使用这个 fixture 的功能测试的扩展测试套件，我将把它留在测试模块中，并缩小匹配的范围。

### 模仿对象以简化单元测试

为了编写代码的单元测试，我们需要找到一种替代方法来启动 aiohttp 库要连接的服务器。如果我们使用 requests 库发出 HTTP 请求，我们可能会使用 *responses* 测试工具，该工具会修补请求内部的某些部分，以允许覆盖特定的 URL。

如果我们的`get_data_points(...)`实现是同步的，我们将注册我们想要用响应覆盖的 URL，并确保为测试方法激活了包。使用响应的测试函数，比如如下所示的假设函数，不会以牺牲可读性为代价引入过多的复杂性。

```py
@responses.activate
def test_get_data_points(self, mut, data) -> None:
    responses.add(responses.GET, 'http://localhost/v/2.0/sensors/',
              json=data, status=200)
    datapoints = mut("http://localhost", "")
    assert len(datapoints) == len(data["sensors"])
    for sensor in data["sensors"]:
        assert sensor["value] in (datapoint.data for datapoint in datapoints)
        assert sensor["id"] in (datapoint.sensor_name for datapoint in datapoints)

```

我们希望能够为 aiohttp 库做一些类似的事情，但是我们有一点优势，因为我们的函数期望将一个 http 客户端对象传递给`get_data_points(...)`函数。我们可以编写一个模拟版本的`ClientSession`对象，它的行为与真实对象非常相似，允许我们注入假数据，而不必像 responses 那样修补真实的实现。

对于简单的对象，我们经常使用标准库中内置的`unittest.mock`功能。模仿允许我们实例化对象并定义各种操作的结果。我们需要的对象有一个`get(...)`方法，它返回一个上下文管理器。这个上下文管理器的 enter 方法返回一个响应对象，它有一个`status`属性和一个`json()`协程，这是一组相对复杂的需求。清单 [8-3](#PC11) 展示了一个使用`unittest.mock`构建这个对象的夹具。

```py
from unittest.mock import Mock, MagicMock, AsyncMock

import pytest

@pytest.fixture
def data() -> t.Any:
    return {
        "sensors": [
            {
                "human_readable": "3.7",
                "id": "PythonVersion",
                "title": "Python Version",
                "value": [3, 7, 2, "final", 0],
            },
            {
                "human_readable": "Not connected",
                "id": "ACStatus",
                "title": "AC Connected",
                "value": False,
            },
        ]
    }

@pytest.fixture
def mockclient(data):
    client = MagicMock()
    response = Mock()
    response.json = AsyncMock(return_value=data)
    response.status = 200
    client.get.return_value.__aenter__ = AsyncMock(return_value=response)
    return client

Listing 8-3Using unittest’s mocking to mock a complex object

```

这个对象不太容易推理:`mockclient`中的代码相当密集，它依赖于理解不同类型的可用模拟类之间的差异，以及上下文管理器的实现。您不能一眼看出如何从测试夹具中使用这个对象。

我们可以通过创建定制类来编写相同的功能，这些定制类反映了我们想要替换的真实类的功能，如清单 [8-4](#PC12) 所示。这种方法导致代码非常长，所以一些开发人员更喜欢前面提到的通用模仿方法。

```py
import contextlib
from dataclasses import dataclass
import typing as t

import pytest

@pytest.fixture
def data() -> t.Any:
    return {
        "sensors": [
            {
                "human_readable": "3.7",
                "id": "PythonVersion",
                "title": "Python Version",
                "value": [3, 7, 2, "final", 0],
            },
            {
                "human_readable": "Not connected",
                "id": "ACStatus",
                "title": "AC Connected",
                "value": False,
            },
        ]
    }

@dataclass
class FakeAIOHttpClient:
    data: t.Any

    @contextlib.asynccontextmanager
    async def get(self, url: str, headers: t.Optional[t.Dict[str, str]]=None) -> FakeAIOHttpResponse:
        yield FakeAIOHttpResponse(json_data=self.data, status=200)

@dataclass
class FakeAIOHttpResponse:
    json_data: t.Any
    status: int

    async def json(self) -> t.Any:
        return self.json_data

@pytest.fixture
def mockclient(data) -> FakeAIOHttpClient:
    return FakeAIOHttpClient(data)

Listing 8-4Manually mocking a complex object

```

使用这种方法的设置时间大约是两倍，但是一眼就能看出所涉及的对象是什么要容易得多。这两种方法之间的差异很大程度上是个人偏好的差异。就我个人而言，在大多数情况下我更喜欢第二种方法，因为我觉得它有一些具体的优点。

`unittest.mock`方法为所有属性访问创建模拟。这可能会引入微妙的测试错误，因为代码可能会开始依赖于一个新的属性，而这在默认情况下会被模拟出来。例如，如果我们编写了一些使用了`if response.cookies:`的代码，那么第一种模拟方法将总是在模拟会话中对`True`求值，但是第二种方法将引发`AttributeError`。我通常更愿意知道我的模仿是不完整的，通过异常，而不是不正确的行为。

然后，当编写包含分支逻辑的模拟时，前一种方法更难使用。它们非常适合断言遵循了什么代码路径，但是不太适合根据情况返回不同的数据。例如，如果我们想要一个模拟会话，它可以为不同的 URL 返回不同的数据，那么对定制对象的更改就相对清楚了。使用模拟对象时的等效变化要复杂得多。

#### 带有分支逻辑的模拟

要使用`Fake*`对象引入每个 url 的模拟响应，只需要修改`FakeAIOHttpClient`类及其在`mockclient`中的调用，这些修改是非常标准的 Python 逻辑。

```py
@dataclass
class FakeAIOHttpClient:
    responses: t.Dict[str, str]

    @contextlib.asynccontextmanager
    async def get(self, url: str, headers: t.Optional[t.Dict[str, str]]=None) -> FakeAIOHttpResponse:
        if url in self.responses:
            yield FakeAIOHttpResponse(json_data=self.responses[url], status=200)
        else:
            yield FakeAIOHttpResponse(json_data=None, status=404)

```

然而，对基于 unittest 的模拟系统的等效更改需要更多的支持代码，并且需要对一些工作进行重构，以更类似于我们的定制模拟方法。

```py
def FakeAIOHTTPClient(response_data):
    client = Mock()
    def find_response(url):
        get_request = MagicMock()
        response = Mock()
        if url in response_data:
            response.json = AsyncMock(return_value=response_data[url])()
            response.status = 200
        else:
            response.json = AsyncMock(return_value=None)()
            response.status = 404
        get_request.__aenter__ = AsyncMock(return_value=response)
        return get_request
    client.get = find_response
    return client

@pytest.fixture
def mockclient(data):
    return FakeAIOHTTPClient({
        "http://localhost/v/2.0/sensors/": data
    })

```

#### 数据类别

你可能已经注意到了前面几个类中的`@dataclass`装饰，因为我们还没有用到它们。数据类是 3.7 版本中引入的 Python 特性。它们大致相当于旧版本 Python 中广泛使用的命名元组特性；它们是定义数据容器的一种方式，可以最大限度地减少所需的样板文件。

通常，当定义一个类来存储数据时，我们必须定义一个`__init__(...)`方法来获取参数(可能带有默认值),然后将这些参数设置为实例属性。每个字段名出现三次，一次在参数列表中，一次在赋值操作的两边，例如，我们的假响应对象的以下变体，它只存储两条数据:

```py
class FakeAIOHttpResponse:
    def __init__(self, body: str, status: int):
        self.body = body
        self.status = status

```

许多 Python 开发人员都非常熟悉这种类结构，因为我们经常需要创建存储结构化数据的方法，这些方法使用属性访问来检索字段。`collections.namedtuple(...)`函数是一种以声明方式实现这一点的方法:

```py
import collections

FakeAIOHttpResponse = collections.namedtuple("FakeAIOHttpResponse", ["body", "status"])

```

除了减少声明只包含样板代码的类的需要之外，这样做还有一个好处，即确保返回对象的有用文本表示，以及像`==`和`!=`这样的比较操作符的行为符合预期。我们前面提到的原始类不比较类上的值，所以`FakeAIOHttpResponse("", 200) == FakeAIOHttpResponse("", 200)`用类版本评估为 False，用命名的元组版本评估为 True。

命名元组是一种特殊类型的元组；可以使用带有字段名称的属性访问或带有索引的项目访问来访问项目。即对于一个`FakeAIOHttpResponse`、`x.body == x[0]`的实例。最后，它们提供了一个`_asdict()`实用方法，该方法返回一个字典，其中包含与命名元组实例相同的数据。

命名元组的最大缺点是它们不容易添加方法。可以对命名元组进行子类化，并以这种方式添加方法，但我不建议这样做，因为可读性较差。

```py
class FakeAIOHttpResponse(collections.namedtuple("", ["body", "status"])):
    async def json(self) -> t.Any:
        return json.loads(self.body)

```

这就是数据类的闪光点。通过在类定义上使用`@dataclasses.dataclass` decorator，可以将一个类变成一个数据类。使用类型语法定义字段，可以选择使用默认值。dataclass decorator 负责将这些类变量转换成定制的`__init__(...)`、`__repr__()`、`__eq__(...)`和其他方法。

```py
@dataclass
class FakeAIOHttpResponse:
    body: str
    status: int = 200

    async def json(self) -> t.Any:
        return json.loads(self.body)

```

Tip

有时候，除了存储值之外，您还想在`__init__`方法中添加其他代码。您可以通过定义一个`__post_init__`方法来对数据类执行此操作，该方法将在`__init__`中的样板文件完成后被调用。

尽管数据类提供了许多与命名元组相同的特性，但它们并不完全与命名元组提供的 API 兼容。它们不实现条目访问、 <sup>[6](#Fn6)</sup> ，并且到字典和元组的转换是通过`dataclasses.asdict(...)`和`dataclasses.astuple(...)`函数完成的，而不是通过类本身的方法。

数据类相对于命名元组的另一个优势是它们是可变的，尽管我们在这里没有用到。在数据类对象被实例化之后，可以改变它的属性值。命名元组就不一样了。此功能是可选的；用`@dataclass(frozen=True)`定义的类不支持在实例化后改变属性。冻结一个数据类的好处是它也可以被哈希(T2)，这意味着它可以作为集合的一部分或者字典的键来存储。

Caution

尽管被冻结的数据类不允许它们的值被*替换为*，但是如果其中一个值是可变的，那么这个字段有可能被就地改变。如果你使用列表、集合或字典作为值类型，我不推荐使用`frozen=True`选项。

还有一些其他的选项可以传递给`@dataclass`装饰器:`eq=False`抑制等式函数的生成，这样相同值的实例就不会相等。或者，传递`order=True`会额外生成丰富的比较字段，其中对象的排序与它们的值的元组一样，按顺序排列。

对于一些高级用例，可以指定每个字段的元数据。例如，我们可能希望响应的 repr 看起来像`FakeAIOHttpResponse(url='http://localhost', status=200)`，也就是说，添加一个 URL 项并从 repr 中省略主体。我们可以通过使用一个`field`对象来做到这一点，这与编写自定义`__repr__()`方法的标准方法相反。两种方法的比较如表 [8-2](#Tab2) 所示。

表 8-2

使用和不使用 dataclass 助手的自定义 repr 行为的比较

<colgroup><col class="tcol1 align-left"> <col class="tcol2 align-left"></colgroup> 
| *使用字段(...)自定义默认 repr*`from dataclasses import dataclass, field``@dataclass``class FakeAIOHttpResponse:``url: str``body: str = field(repr=False)``status: int = 200``async def json(self) -> t.Any:``return json.loads(self.body)` | *使用自定义 __repr__*`from dataclasses import dataclass``@dataclass``class FakeAIOHttpResponse:``url: str``body: str``status: int = 200``def __repr__(self):``name = type(self).__name__``url = self.url``status = self.status``return f"{name}({url=}, {status=})"``async def json(self) -> t.Any:``return json.loads(self.body)` |

`field(...)`方法的优点是明显更短，尽管稍微不太直观。`__repr__()`方法允许完全控制，代价是需要重新实现默认行为。

在某些情况下，field 方法是强制的:支持默认为可变对象的字段，比如 list 或 dict。这与建议不要使用可变对象作为函数的默认值是出于同样的原因，因为它们被就地修改会导致数据在实例间溢出。

字段对象接受一个`default_factory`参数，这是一个可调用的参数，为每个实例生成默认值。这可以是用户指定的函数，也可以是不带参数的类构造函数。

```py
options: t.List[str] = field(default_factory=list)

```

##### 上下文库

与我们使用`yield`分割 pytest fixture 的安装和拆卸部分一样，我们可以使用标准库中的`contextlib`的装饰器来创建上下文管理器，而不必显式实现`__enter__()`和`__exit__(...)`方法对。

装饰器是创建上下文管理器最简单的方法，尤其是我们在这里使用的非常简单的方法。上下文管理器最常见的用途是创建一些资源，并确保它在之后被正确清理。表 [8-3](#Tab3) 显示，如果我们正在制作一个上下文管理器，其行为方式与之前的 HTTP 服务器 fixture 相同，那么代码几乎是相同的。

表 8-3

具有拆卸功能的 pytest fixture 与上下文管理器的比较

<colgroup><col class="tcol1 align-left"> <col class="tcol2 align-left"></colgroup> 
| *Pytest fixture 创建 HTTP 服务器*`import pytest``@pytest.fixture(scope="module")``def http_server():``yield from run_server_in_thread(``"standard", {``"APD_SENSORS_API_KEY": "testing"``}, 12081``)` | *上下文管理器创建一个 HTTP 服务器*`import contextlib``@contextlib.contextmanager``def http_server():``yield from run_server_in_thread(``"standard", {``"APD_SENSORS_API_KEY": "testing"``}, 12081``)` |

更复杂的上下文管理器，比如需要处理发生在它们包装的代码中的异常的上下文管理器，需要将`yield`语句视为可能引发异常的语句。因此，`yield`语句通常应该在`try` / `finally`块或`with`块中，以确保任何资源都被正确地拆除。

`FakeAIOHttpClient`上的`get(...)`方法是异步上下文管理器，而不是标准上下文管理器。`@contextlib.contextmanager`装饰器从生成器方法中创建`__enter__()`和`__exit__(...)`方法；我们需要的是一个装饰器来从一个生成器协程创建`__aenter__()`和`__aexit__(...)`协程。这可以作为`@contextlib.asynccontextmanager`装饰器获得。

#### 测试方法

既然我们已经准备好支持代码的快速集成测试，我们就可以开始编写实际的测试函数了。首先，我们可以在没有 HTTP 服务器开销的情况下验证`get_data_points(...)`方法的行为。 <sup>[7](#Fn7)</sup> 然后我们可以根据`get_data_points(...)`为`add_data_from_sensors(...)`方法添加测试。最后，我们需要测试来确保应用程序的数据库部分正常工作，我们仍然需要修改它来消除阻塞行为。

清单 [8-5](#PC20) 中显示的测试方法结合了我们目前使用的技术。对`get_data_points(...)`的测试使用定制对象生成的`mockclient`。这是所有依赖于 HTTP 库的准确行为的一组测试中的第一个。另一方面，`add_data_from_sensors`测试使用一个`unittest.mock.Mock()`对象来模拟数据库会话，因为我们只需要断言某些方法在我们期望的时候被调用。

`patch_aiohttp()`夹具结合了这两种方法，以及夹具的安装和拆卸功能。只要上下文管理器是活动的，`unittest.mock.patch(...)`上下文管理器就获取一个 Python 对象的位置并用一个 mock 替换它。由于`add_data_from_sensors(...)`方法不接受`ClientSession`作为参数，所以我们不能将自定义的模拟传递给它。这允许我们将我们的定制模拟方法移植到 aiohttp 库，每当我们的测试代码创建一个`ClientSession`时就返回，就像 responses 对 requests 库所做的那样。

```py
from unittest.mock import patch, Mock, AsyncMock

import pytest

import apd.aggregation.collect

class TestGetDataPoints:
    @pytest.fixture
    def mut(self):
        return apd.aggregation.collect.get_data_points

    @pytest.mark.asyncio
    async def test_get_data_points(
        self, mut, mockclient: FakeAIOHttpClient, data
    ) -> None:
        datapoints = await mut("http://localhost", "", mockclient)

        assert len(datapoints) == len(data["sensors"])
        for sensor in data["sensors"]:
            assert sensor["value"] in (datapoint.data for datapoint in datapoints)
            assert sensor["id"] in (datapoint.sensor_name for datapoint in datapoints)

class TestAddDataFromSensors:
    @pytest.fixture
    def mut(self):
        return apd.aggregation.collect.add_data_from_sensors

    @pytest.fixture(autouse=True)
    def patch_aiohttp(self, mockclient):
        # Ensure all tests in this class use the mockclient
        with patch("aiohttp.ClientSession") as ClientSession:
            ClientSession.return_value.__aenter__ = AsyncMock(return_value=mockclient)
            yield ClientSession

    @pytest.fixture
    def db_session(self):
        return Mock()

    @pytest.mark.asyncio
    async def test_datapoints_are_added_to_the_session(self, mut, db_session) -> None:
        # The only times data should be added to the session are when # running the MUT
        assert db_session.add.call_count == 0
        datapoints = await mut(db_session, ["http://localhost"], "")
        assert db_session.add.call_count == len(datapoints)

Listing 8-5The various approaches of test methods for apd.aggregation

```

最终的测试并不过分复杂，并且覆盖了与功能测试相同的一般功能。它们为未来的测试提供了一个基础，功能测试提供了一个退路，让我们确信我们的测试有有用的断言。这里的集成测试都是*阳性*，确认正常情况下有效。我们还没有任何证据证明不寻常的或边缘的情况得到了正确的处理，但它们是一个很好的起点。

## 异步数据库

到目前为止，我们一直使用 SQLAlchemy ORM 来处理数据库和 Python 代码之间的所有交互，因为它允许将数据库的许多特性放到一边，以支持看起来正常的 Python 代码。不幸的是，SQLAlchemy ORM 不适合在纯异步环境中使用。SQLAlchemy 不保证 SQL 查询只在响应`session.query(...)`调用时运行；访问对象的属性时也可以运行查询，更不用说插入和事务管理查询了。所有这些调用都会阻塞执行，严重影响 asyncio 应用程序的性能。

这并不意味着 SQLAlchemy ORM 在异步上下文中运行时会更慢；阻塞通常是最小的，并且仍然存在于 SQLAlchemy 的同步使用中。相反，这意味着在异步代码中使用 SQLAlchemy ORM 会导致性能下降到与同步代码相同的水平，从而抵消了使用 asyncio 的许多好处。

如果我们愿意牺牲 SQLAlchemy 的 ORM 组件，只将其用作 SQL 语句生成器和接口，就不会出现无意查询的风险。这是一个真正的损失，是我们到目前为止考虑的与使我们的代码异步相关的最大损失，因为 SQLAlchemy ORM 是一个设计如此良好的库。

在撰写本文时，数据库连接还没有完美的解决方案；但是，我觉得语句生成方法是一个很好的折衷方案。只要您没有编写异步服务器应用程序，并且能够承受性能下降的风险，您就应该考虑使用 ORM 的实用方法，尽一切努力避免在主线程中调用阻塞代码。

### 经典的 SQLAlchemy 风格

在我们的例子中，我们将使用语句生成方法。我们不能继续使用之前创建的基于`declarative_base`的类，因为这可能会无意中触发 SQL 查询。使用“经典”样式(即，不是直接从它们所代表的 Python 类派生的显式表对象)并且不配置 ORM 来链接表和我们的 Python 对象，这让我们可以安全地使用`DataPoint`对象，而不会触发隐式查询。清单 [8-6](#PC21) 中给出了我们现有表格的实现。

这种方法意味着我们将不会直接在数据库层处理我们的自定义对象，我们将处理表，并将负责我们的对象和 SQLAlchemy API 之间的转换。然而，我们只是改变了我们表示数据库的方式，而不是数据库结构，所以我们不需要为这种改变创建任何迁移。

```py
from dataclasses import dataclass, field
import datetime
import typing as t

import sqlalchemy
from sqlalchemy.dialects.postgresql import JSONB, TIMESTAMP
from sqlalchemy.schema import Table

metadata = sqlalchemy.MetaData()

datapoint_table = Table(
    "sensor_values",
    metadata,
    sqlalchemy.Column("id", sqlalchemy.Integer, primary_key=True),
    sqlalchemy.Column("sensor_name", sqlalchemy.String),
    sqlalchemy.Column("collected_at", TIMESTAMP),
    sqlalchemy.Column("data", JSONB),
)

@dataclass
class DataPoint:
    sensor_name: str
    data: t.Dict[str, t.Any]
    id: int = None
    collected_at: datetime.datetime = field(default_factory=datetime.datetime.now)

Listing 8-6The “classic” style, with independent table and data classes

```

在我们做任何事情之前，我们应该更新我们的`alembic/env.py`脚本，因为它需要引用`metadata`对象来生成迁移。之前是导入`Base`，然后接入`Base.metadata`；我们必须修改这些行来使用我们的新元数据对象，`apd.aggregation.database.metadata`。

我们不能再通过实例化一个`DataPoint`对象并将其添加到会话中来创建数据库记录；相反，我们直接对`datapoint_table`结构进行插入调用。

```py
stmt = datapoint_table.insert().values(
    sensor_name="ACStatus",
    collected_at=datetime.datetime(2020,4,1,12,00,00),
    data=False
)
session.execute(stmt)

```

`stmt`对象是 SQLAlchemy 中`Insert`的一个实例。此对象代表要执行的 SQL 语句的结构；它不是直接传递给数据库的字符串。虽然可以查看表示语句的字符串，但是我们需要指定它用于哪种数据库，以便获得准确的结果。这是 SQLAlchemy 通过基于连接信息的`stmt.compile(dialect=...)`方法调用在内部完成的。不同数据库的 SQL 标准和指定插值的方式略有不同；编译步骤是应用特定于数据库的语法。作为防止 SQL 注入漏洞工作的一部分，所有的变体都将从 SQL 结构中传递的值分开。

#### 未编译

```py
INSERT INTO datapoints (sensor_name, collected_at, data) VALUES (:sensor_name, :collected_at, :data)
{'sensor_name': 'ACStatus', 'collected_at': datetime.datetime(2020, 4, 1, 12, 0), 'data': False}

```

#### 数据库

```py
INSERT INTO datapoints (sensor_name, collected_at, data) VALUES (:sensor_name, :collected_at, :data)

{'sensor_name': 'ACStatus', 'collected_at': datetime.datetime(2020, 4, 1, 12, 0), 'data': False}

```

#### 关系型数据库

```py
INSERT INTO datapoints (sensor_name, collected_at, data) VALUES (%s, %s, %s)
['ACStatus', datetime.datetime(2020, 4, 1, 12, 0), False]

```

#### 一种数据库系统

```py
INSERT INTO datapoints (id, sensor_name, collected_at, data) VALUES (%(id)s, %(sensor_name)s, %(collected_at)s, %(data)s)
{'id': None, 'sensor_name': 'ACStatus', 'collected_at': datetime.datetime(2020, 4, 1, 12, 0), 'data': False}

```

#### 数据库

```py
INSERT INTO datapoints (sensor_name, collected_at, data) VALUES (?, ?, ?)
['ACStatus', datetime.datetime(2020, 4, 1, 12, 0), False]

```

除了好奇，我们不需要看这些字符串，也不需要手动编译 insert 语句。我们通过 SQLAlchemy 建立的会话在使用`session.execute(stmt)`执行时直接处理一个`Insert`对象。

这个`execute(...)`方法将语句发送到数据库并等待响应。例如，如果有一个 SQL 锁需要等待，这个 Python 语句就可以阻塞。`session.commit()`调用也可能导致阻塞，因为这是前面的插入命令被终结的地方。简而言之，使用这种方法，我们需要确保任何涉及会话的调用总是发生在不同的线程中。

忽略 SQL 生成的细节而只调用`table.insert().values(...)`的能力展示了我们通过使用 SQLAlchemy 保留的一些优势，即使是以这种更有限的方式。通过编写在两种数据类型之间转换的实用函数，我们可以做得更好。我们最初可能会尝试使用`**dataclasses.asdict(...)`来生成`values(...)`调用的主体，但这将包括`id=None`。我们不想在 SQL insert 中将 id 设置为`None`，我们想从参数列表中省略它，以便数据库设置它。为了使这更容易，我们将在数据类(清单 [8-7](#PC28) )上创建一个调用`asdict(self)`的函数，但该函数只包含显式设置的 id。

```py
from dataclasses import dataclass, field, asdict
import datetime
import typing as t

@dataclass
class DataPoint:
    sensor_name: str
    data: t.Dict[str, t.Any]
    id: int = None
    collected_at: datetime.datetime = field(default_factory=datetime.datetime.now)

    def _asdict(self):
        data = asdict(self)
        if data["id"] is None:
            del data["id"]
        return data

Listing 8-7Implementation of DataPoint class with a helper method for database queries

```

### 使用运行执行程序

我们在前一章简单讨论了`run_in_executor(...)`函数，以允许`time.sleep(1)`与`asyncio.sleep(1)`并行运行而不是顺序运行为例。这是一个相当不自然的例子，但是将数据库调用转移到一个新的线程中非常适合。

Caution

`run_in_executor(...)`方法与我们之前使用的`with ThreadPoolExecutor()`结构不可互换。两者都将工作委托给一个线程；池执行器构造建立一个池，提交工作，然后等待所有工作完成，而`run_in_executor(...)`方法创建一个长时间运行的池，允许您提交任务并等待来自异步代码的值。

到目前为止，我们使用的许多 asyncio 帮助函数，如`asyncio.gather(...)`、`asyncio.create_task(...)`和`asyncio.Lock()`，都会自动检测当前的 asyncio 事件循环。`run_in_executor(...)`功能有点不一样；它只能作为事件循环实例上的方法使用。我们需要用`asyncio.get_running_loop()`自己获取当前事件循环，然后用它来提交要在执行器中运行的函数。我建议提交一个同步任务来完成您需要的所有工作，而不是为每个低级调用提交单独的任务并用 asyncio 逻辑将它们粘合在一起，例如，创建一个为一组对象生成插入查询的`handle_result(...)`函数(清单 [8-8](#PC29) ),而不是为每个要插入的对象创建一个函数调用。

```py
def handle_result(result: t.List[DataPoint], session: Session) -> t.List[DataPoint]:
    for point in result:
        insert = datapoint_table.insert().values(**point._asdict())
        sql_result = session.execute(insert)
        point.id = sql_result.inserted_primary_key[0]
    return result

async def add_data_from_sensors(
    session: Session, servers: t.Tuple[str], api_key: t.Optional[str]
) -> t.List[DataPoint]:
    tasks: t.List[t.Awaitable[t.List[DataPoint]]] = []
    points: t.List[DataPoint] = []
    async with aiohttp.ClientSession() as http:
        tasks = [get_data_points(server, api_key, http) for server in servers]
        for results in await asyncio.gather(*tasks):
            points += results
    loop = asyncio.get_running_loop()
    await loop.run_in_executor(None, handle_result, points, session)
    return points

Listing 8-8Database integration function for adding data points

```

`loop.run_in_executor`的参数是`(executor, callable, *args)`，其中`executor`必须是`ThreadPoolExecutor`或`None`的一个实例(使用默认的执行程序，必要时创建它)。

Tip

如果您正在适应大量的同步任务，我建议您直接管理线程池。这将允许您设置他们的工人数量，从而设置他们将执行的同时任务的数量。这还将允许您在决定需要添加什么锁来使代码线程安全时，更有效地推理哪些代码可以同时执行。

在这个执行器中，`callable`函数将作为一个任务被调用，其位置参数在`*args`中指定。您不能将关键字参数作为此 API 的一部分指定给 callable。

使用需要关键字参数的函数的最佳方式是使用`functools.partial(...)`函数。它将一个函数转换成另一个参数更少的函数。如果我们将`handle_result(...)`函数包装在一个分部函数中，如下所示，那么下面的函数调用将是等效的:

```py
>>> only_points = functools.partial(handle_result, session=Session)
>>> only_session = functools.partial(handle_result, points=points)
>>> no_args = functools.partial(handle_result, points=points, session=Session)

>>> handle_result(points=points, session=Session)
[DataPoint(...), DataPoint(...)]

>>> only_points(points=points)
[DataPoint(...), DataPoint(...)]

>>> only_session(session=Session)
[DataPoint(...), DataPoint(...)]

>>> no_args()
[DataPoint(...), DataPoint(...)]

```

除了像`run_in_executor(...)`这样不支持关键字参数的 API 之外，在传递函数时使用一些参数集而不使用其他参数集有时是很有用的，例如，不需要将数据库会话或 web 请求传递给每个函数。

Django’s ORM

许多从事 Web 工作的 Python 开发人员会在职业生涯的某个阶段使用 Django，他们可能想知道从异步代码(比如从通道)与 Django ORM 交互的等效过程是什么。

我对 Django 的建议是像平常一样使用 ORM，但是只能从同步函数中使用。您可以使用实用程序方法`@channels.db.database_sync_to_async`调用同步函数，该方法可以用作同步函数的修饰器，使它们成为可调用的。这个装饰器通过一个显式的线程池委托给`run_in_executor(...)`,但是也执行一些特定于 Django 的数据库连接管理。

```py
from channels.db import database_sync_to_async

@database_sync_to_async
def handle_result(result: t.List[t.Dict[str, t.Any]]) -> t.List[DataPoint]:
    points: t.List[DataPoints] = []
    for data in result:
        point = DataPoint(**data)
        point.save()
        points.append(point)
    return points

```

如果在 Django 通道的上下文中使用假设的`handle_result(...)`，前面的代码将是一个示例。由于 Django 强烈建议在给出响应之前提前执行所有的数据收集操作，这是一个次优但可行的解决方案。

### 查询数据

使用 SQLAlchemy 的 ORM 时，查询数据和接收 Python 对象是一件简单的事情。尽管如此，由于我们只使用了 SQLAlchemy 的查询构建和执行部分，这有点复杂。在支持 ORM 的 SQLAlchemy 中，我们会找到 PythonVersion 传感器的所有`DataPoint`条目

```py
db_session.query(DataPoint).filter(DataPoint.sensor_name=="PythonVersion")

```

但是我们需要使用 table 对象，并从`c`属性中引用它的列，如下所示:

```py
db_session.query(datapoint_table).filter(datapoint_table.c.sensor_name=="PythonVersion")

```

我们拿回来的对象不是`DataPoint`对象，而是 SQLAlchemy 自己内部的命名元组实现，叫做轻量级命名元组。对于没有设置类映射器的任何查询，都将返回这些。

这些内部命名元组提供了一个`_asdict()`方法，因此将`result`对象转换为`DataPoint`对象的最佳方式是`DataPoint(**result._asdict()).`不幸的是，这些对象是动态生成的，被认为是 SQLAlchemy 的实现细节。因此，我们不能在函数的类型定义中使用这些对象。一旦我们添加了一个用于将命名元组转换为数据类的帮助器方法，我们的最终代码与清单 [8-9](#PC34) 相同。

```py
from dataclasses import dataclass, field, asdict
import datetime
import typing as t

@dataclass
class DataPoint:
    sensor_name: str
    data: t.Dict[str, t.Any]
    id: int = None
    collected_at: datetime.datetime = field(default_factory=datetime.datetime.now)

    @classmethod
    def from_sql_result(cls, result):
        return cls(**result._asdict())

    def _asdict(self):
        data = asdict(self)
        if data["id"] is None:
            del data["id"]
        return data

Listing 8-9Final implementation of DataPoint class that supports manual object mapping to SQLAlchemy

```

我们现在可以使用 SQLAlchemy 进行查询，这些查询返回我们的对象，但是结果对象与数据库没有任何直接连接，这可能会导致发出意外的查询。

```py
results = map(
    DataPoint.from_sql_result,
    db_session.query(datapoint_table).filter(datapoint_table.c.sensor_name=="PythonVersion")
)

```

我们也可以在编写测试时使用这种方法，使它们几乎和使用 ORM 风格的相同代码一样清晰。

```py
    @pytest.mark.asyncio
    async def test_datapoints_can_be_mapped_back_to_DataPoints(
        self, mut, db_session, table, model
    ) -> None:
        datapoints = await mut(db_session, ["http://localhost"], "")
        db_points = [
            model.from_sql_result(result) for result in db_session.query(table)
        ]
        assert db_points == datapoints

```

Tip

如果您正在使用 Pandas 数据分析框架，DataFrame 对象提供了加载和存储来自 SQLAlchemy 查询的信息的专用方法。这些`read_sql(...)`和`to_sql(...)`方法在加载大型数据集时非常有用。

### 避免复杂的查询

经常可以看到人们在 ORM 中构建非常复杂的查询，比如涉及多个连接、 <sup>[8](#Fn8)</sup> 条件和子查询的查询。有几个技巧可以让我们更容易理解代表复杂条件的代码。对于 SQLAlchemy 来说，这是`@hybrid_property`特性，而对于 Django 来说，这相当于定制查找和转换。

在第 [6](06.html) 章中，我们看了 SQLAlchemy 如何改变映射类中类属性的行为，使得列可以表示字段的值，或者 SQL 可以表示列，这取决于属性访问是在类的实例上进行的还是在类本身上进行的。混合属性允许将相同的方法扩展到您的定制逻辑。

这里的好处是重新组织代码，所以为了演示它在哪里有用，我们首先需要一个受益于重构的特性需求。我们很可能想要查看某一天常见值的汇总。显示传感器名称、它们的不同值以及今天发生的所有条目的值被看到的次数的查询可以在 SQLAlchemy 中表示为非常长的查询:

```py
value_counts = (
    db_session.query(
        datapoint_table.c.sensor_name,
        datapoint_table.c.data,
        sqlalchemy.func.count(datapoint_table.c.id)
    )
    .filter(
        sqlalchemy.cast(datapoint_table.c.collected_at, DATE)
        == sqlalchemy.func.current_date()
    )
    .group_by(datapoint_table.c.sensor_name, datapoint_table.c.data)
)

```

这有几个问题。首先，`name`和`data`列出现了两次，因为我们希望根据它们进行分组，但是我们还需要能够看到哪个结果与哪个分组相关联，因此它们也必须出现在输出列中。其次，我们得到的过滤器很复杂，既要读取又要执行。读取很困难，因为它涉及到对 SQLAlchemy 函数的多次调用，而不是简单的比较。执行起来很困难，因为我们正在用强制转换修改`collected_at`属性，这会使该列上的任何索引无效(如果我们已经设置了任何索引的话)。

Note

我用`sqlalchemy.func.current_date()`来表示当前日期。数据库中任何可用的函数都可以通过`sqlalchemy.func`按名称访问。这纯粹是一种风格选择；使用`datetime.date.today()`或其他任何被数据库解释为日期的东西并不会更快或更慢。

查看 PostgreSQL 如何解释查询的最简单方法是打开一个数据库 shell，并在那里用`EXPLAIN ANALYZE`修饰符运行查询。 <sup>[9](#Fn9)</sup> 输出格式相当复杂，但是有很多 PostgreSQL 的资源深入讲解了如何阅读它们以及优化方法。

目前，我们的目标是创建一个既易读又不会不必要地慢的查询。首先，让我们将公共列移到变量中以减少重复。

```py
headers = datapoint_table.c.sensor_name, datapoint_table.c.data
value_counts = (
    db_session.query(*headers, sqlalchemy.func.count(datapoint_table.c.id))
    .filter(
        sqlalchemy.cast(datapoint_table.c.collected_at, DATE)
        == sqlalchemy.func.current_date()
    )
    .group_by(*headers)
)

```

这使得滤波器部分成为速度和可读性的瓶颈。我建议的下一步是在底层表中的`collected_at`和`sensor_name`字段上添加一些索引。我们通过将`index=True`添加到表上的字段并生成一个新的 alembic 修订来实现这一点，如下所示:

```py
datapoint_table = Table(
    "datapoints",
    metadata,
    sqlalchemy.Column("id", sqlalchemy.Integer, primary_key=True),
    sqlalchemy.Column("sensor_name", sqlalchemy.String, index=True),
    sqlalchemy.Column("collected_at", TIMESTAMP, index=True),
    sqlalchemy.Column("data", JSONB),
)

> pipenv run alembic revision --autogenerate -m "Add indexes to datapoints"
> pipenv run alembic upgrade head

```

不幸的是，这不足以改变我们的执行计划，因为作为比较的一部分，我们正在操作`collected_at`列。这使得索引无效，因为`CAST()`函数的结果不是索引可以缓存的操作之一。可以在数据库中创建一个函数，返回给定时间戳的日期，并对该函数的结果进行索引，但是这种方法不会使我们的代码更容易阅读。

相反，我建议使用`@hybrid_property`将这个条件分解到类的一个属性中。我们可以复制相同的条件，但这只会使代码更容易阅读，而不是更有效地执行。将该条件分解出来的一个优点是可读性和效率之间的平衡发生了变化:如果它隐藏在一个具有有用名称的实用函数后面，而不是分散在整个代码库中，那么我们可以拥有一个更有效但可读性更差的条件。

除了具有可选的`expression=`、`update_expression=`和`comparator=`属性之外，`@hybrid_property`装饰器的工作方式与标准的`@property`装饰器相似。一个`expression`是一个类方法，它返回一个*可选择的*(即表示 SQLAlchemy 值的东西)，比如`CAST(datapoint_table.c.collected_at, DATE)`。`update_expression`是一个类方法，它接受一个值并返回列的 2 元组列表和它们的新值，作为`expression`的逆操作，允许更新列。这两种方法允许柱的外观与原生柱的行为相同。混合属性通常用于全名之类的东西，用来连接名和姓。 <sup>[10](#Fn10)</sup> 通常只有`expression`被实现，而没有`update_expression`。在这种情况下，该属性是只读的。

`comparator`属性有一点不同:它不能与`expression`或`update_expression`特性结合使用，但是它允许实现更复杂的情况，比较操作符的两个部分都可以在发送到数据库之前定制。这种用法通常用于小写电子邮件地址或用户名，尽量使它们不区分大小写。 <sup>[11](#Fn11)</sup>

比较器和表达式不兼容的原因是，`expression`特性是通过使用默认的比较器`ExprComparator`实现的，所以我们不能提供自己的比较器，除非它覆盖处理`expression`的代码。因为我们想要使用这两个特性，我们可以子类化`ExprComparator`来使用它必须委托给表达式的能力，但是也覆盖比较器函数的实现。

我们可以创建一个`@hybrid_property`,将日期时间转换为一个日期，同时使用一个定制的比较器来利用一些特定于数据库的优化。Postgres 将日期视为等同于时间部分为午夜的 datetime。我们可以确保右边是指定日期的午夜或更晚时间，并且在第二天的午夜之前，而不是确保比较的两边都是日期。我们可以通过确保比较的右边是一个日期并加 1 找到第二天来实现这一点。这允许我们使用索引进行两次比较，以获得与不使用索引的一次比较相同的结果。清单 [8-10](#PC42) 中给出了更新的数据点实现。

```py
from __future__ import annotations

from dataclasses import dataclass, field, asdict
import datetime
import typing as t

import sqlalchemy
from sqlalchemy.dialects.postgresql import JSONB, DATE, TIMESTAMP
from sqlalchemy.ext.hybrid import ExprComparator, hybrid_property
from sqlalchemy.orm import sessionmaker
from sqlalchemy.schema import Table

metadata = sqlalchemy.MetaData()

datapoint_table = Table(
    "sensor_values",
    metadata,
    sqlalchemy.Column("id", sqlalchemy.Integer, primary_key=True),
    sqlalchemy.Column("sensor_name", sqlalchemy.String, index=True),
    sqlalchemy.Column("collected_at", TIMESTAMP, index=True),
    sqlalchemy.Column("data", JSONB),
)

class DateEqualComparator(ExprComparator):

    def __init__(self, fallback_expression, raw_expression):
        # Do not try and find update expression from parent
        super().__init__(None, fallback_expression, None)
        self.raw_expression = raw_expression

    def __eq__(self, other):
        """ Returns True iff on the same day as other """
        other_date = sqlalchemy.cast(other, DATE)
        return sqlalchemy.and_(
            self.raw_expression >= other_date,
            self.raw_expression < other_date + 1,
        )

    def operate(self, op, *other, **kwargs):
        other = [sqlalchemy.cast(date, DATE) for date in other]
        return op(self.expression, *other, **kwargs)

    def reverse_operate(self, op, other, **kwargs):
        other = [sqlalchemy.cast(date, DATE) for date in other]
        return op(other, self.expression, **kwargs)

@dataclass

class DataPoint:
    sensor_name: str
    data: t.Dict[str, t.Any]
    id: t.Optional[int] = None
    collected_at: datetime.datetime = field(default_factory=datetime.datetime.now)

    @classmethod
    def from_sql_result(cls, result) -> DataPoint:
        return cls(**result._asdict())

    def _asdict(self) -> t.Dict[str, t.Any]:
        data = asdict(self)
        if data["id"] is None:
            del data["id"]
        return data

    @hybrid_property
    def collected_on_date(self):
        return self.collected_at.date()

    @collected_on_date.comparator
    def collected_on_date(cls):
        return DateEqualComparator(
            cls,
            sqlalchemy.cast(datapoint_table.c.collected_at, DATE),
            datapoint_table.c.collected_at,
        )

Listing 8-10DataPoint table and model, with transparent optimized comparator for dates

```

`ExprComparator`类型的构造函数有三个参数，模型类、表达式和混合属性。`__init__(...)`中的`class=`和`hybrid_property=`参数用于实现更新行为，但是由于我们不需要这个特性，我们将简化接口并将`None`传递给这些参数。expression 参数是我们希望用于查询和任何比较的参数(除非另有说明)。在`__init__(...)`函数中，我们为底层列添加了一个新参数，这样我们就可以在自定义的比较函数中访问原始数据。

`operate(...)`和`reverse_operate(...)`函数实现了各种比较。它们允许对比较双方的参数进行操作，我们需要确保被比较的对象是 PostgreSQL 中的`CAST()`到`DATE`。`__eq__(...)`方法是我们的自定义等式检查器，在这里我们实现了一个更有效的版本来检查两边是否是同一个日期，如前所述。

所有这些的效果是，我们可以无缝地比较两个 datetime 值，并获得正确的结果。两边都是`CAST()`对`DATE`，除非是相等检查(我们试图优化的检查)，在这种情况下，只有参数是`CAST()`对`DATE`，允许左边的列使用索引。表 [8-4](#Tab4) 显示了可能的 Python 表达式、它们被翻译成的 SQL 或 Python，以及是否可以使用索引。

表 8-4

每个操作对混合属性的影响摘要

<colgroup><col class="tcol1 align-left"> <col class="tcol2 align-left"> <col class="tcol3 align-left"></colgroup> 
| 

**Python 表达式**

 | 

**评估结果**

 | 

**使用的索引**

 |
| --- | --- | --- |
| `DataPoint.collected_on_date` | `CAST(sensor_values.collected_at AS DATE)` | 不 |
| `DataPoint(...).collected_on_date` | `datetime.date(2020, 4, 1)` | 不适用(在 Python 中评估) |
| `DataPoint.collected_on_date == other_date` | `sensor_values.collected_at >= CAST(%(param_1)s AS DATE) AND sensor_values.collected_at < CAST(%(param_1)s AS DATE) + %(param_2)s` | 是(仅在处收集，不在右侧收集) |
| `DataPoint.collected_on_date < other_date` | `CAST(sensor_values.collected_at AS DATE) < CAST(%(param_1)s AS DATE)` | 不 |
| `DataPoint(...).collected_on_date == other_date` | `datetime.date(2020, 4, 1) == other_date` | 不适用(在 Python 中评估) |
| `DataPoint(...).collected_on_date < other_date` | `datetime.date(2020, 4, 1) < other_date` | 不适用的(用 Python 评估) |

有了这个`collected_on_date`表达式和比较器，我们可以大大简化查询代码。当阅读代码时，使用这个作为条件更容易理解，并且我们已经确保生成了利用索引的高效 SQL。

```py
headers = table.c.sensor_name, table.c.data
value_counts = (
    db_session.query(*headers, sqlalchemy.func.count(table.c.id))
    .filter(
        model.collected_on_date == sqlalchemy.func.current_date()
    )
    .group_by(*headers)
)

```

Django’s ORM (Redux)

Django 的 ORM 以不同的方式处理这类问题，但是等效的功能确实存在。本小节给出了如何实现这一点的简要说明(对于已经熟悉 Django 的人来说)。有关更多详细信息，请查看本章末尾的其他资源。

Django 没有与`@hybrid_property`或在变量中存储任意 SQL 结构等价的东西。使用查找和转换将代码分解成可重用的组件。

这些在查询中以类似于连接的方式被引用，所以如果前面的代码是 Django 模型，我们将能够使用

```py
DataPoints.objects.filter(collected_at__date=datetime.date.today())

```

这在日期时间字段上使用内置的`date`转换，将日期时间转换为一个日期。定义了一个转换器，用一个`lookup_name`属性指定它可用的名称，用一个`output_field`属性指定它创建的类型。它可以有一个`function`属性(如果它直接映射到一个单参数数据库函数)，或者它可以定义一个定制的`as_sql(...)`方法。

查找的工作方式类似于转换器，但是它不能被链接，因此没有输出类型。它提供了一个`lookup_name`属性和一个`as_sql(...)`方法来生成相关的 SQL。这些也可以通过`__name`访问，如果没有指定其他的，名为`exact`的查找是默认的。

转换器和查找都需要注册才能使用。它们可以根据字段类型或另一个变压器进行注册。如果它们注册在一个字段上，它们将总是在任何具有该类型的表达式上可用，但是如果它们注册在一个转换器上，它们只有在紧跟转换器之后时才有效。我们可以通过在`collected_at__date`中使用的`TruncDate`转换器上定义一个自定义的`exact`查找来构建一个自定义的等式检查，如清单 [8-11](#PC45) 所示。每当我们使用`datetimefield__date`时，这都适用，但在使用本地日期列时不适用。

```py
from django.db import models
from django.db.models.functions.datetime import TruncDate

@TruncDate.register_lookup
class DateExact(models.Lookup):
    lookup_name = 'exact'

    def as_sql(self, compiler, connection):
        # self.lhs (left-hand-side of the comparison) is always TruncDate, we # want its argument
        underlying_dt = self.lhs.lhs
        # Instead, we want to wrap the rhs with TruncDate
        other_date = TruncDate(self.rhs)
        # Compile both sides
        lhs, lhs_params = compiler.compile(underlying_dt)
        rhs, rhs_params = compiler.compile(other_date)
        params = lhs_params + rhs_params + lhs_params + rhs_params
        # Return ((lhs >= rhs) AND (lhs < rhs+1)) - compatible with # postgresql only!
        return '%s >= %s AND %s < (%s + 1)' % (lhs, rhs, lhs, rhs), params

Listing 8-11Implementation of a date comparison in Django’s ORM

```

与 SQLAlchemy 版本一样，这允许在使用`collected_at__date=datetime.date.today()`时进行高效的自定义查找，但是对于`collected_at__date__le==datetime.date.today()`和其他比较，会退回到效率较低的强制转换行为。

#### 根据视图查询

在整个代码库中，很多地方都需要一个很难用 ORM 表示的查询。由于指定连接的方式，这在使用 Django ORM 时稍微常见一些，但是在使用 SQLAlchemy 时确实会发生。一个典型的例子是关联一个表中的多行，特别是按日期或地理位置，而不是与另一个表中的一行相关。例如，一个存储用户和旅行计划并希望查询在给定日期哪些用户对彼此接近的数据库很难在 ORM 中表示。

在这种情况下，您可能会发现创建数据库视图并对其进行查询更容易。它不会改变性能特征， <sup>[12](#Fn12)</sup> ，但确实允许将复杂的查询像表一样处理，大大简化了等式的 Python 一侧。

SQLAlchemy 支持从视图派生的表，因此我们可以使用我们之前创建的查询，将其转换为视图，然后将其作为表映射回 SQLAlchemy。我们可以在数据库控制台中手动创建视图，但是我建议创建一个新的 alembic 版本来发出`CREATE VIEW`语句，这样它就可以更容易地跨实例部署。创建不带`--autogenerate`标志的 alembic 版本，并修改结果文件，如清单 [8-12](#PC46) 所示。

```py
"""Add daily summary view

Revision ID: 6962f8455a6d
Revises: 4b2df8a6e1ce
Create Date: 2019-12-03 11:50:24.403402

"""
from alembic import op

# revision identifiers, used by Alembic.
revision = "6962f8455a6d"
down_revision = "4b2df8a6e1ce"
branch_labels = None
depends_on = None

def upgrade():
    create_view = """
    CREATE VIEW daily_summary AS
      SELECT
        datapoints.sensor_name AS sensor_name,
        datapoints.data AS data,
        count(datapoints.id) AS count
    FROM datapoints
    WHERE
        datapoints.collected_at >= CAST(CURRENT_DATE AS DATE)
        AND
        datapoints.collected_at < CAST(CURRENT_DATE AS DATE) + 1
    GROUP BY
        datapoints.sensor_name,
        datapoints.data;
    """
    op.execute(create_view)

def downgrade():
    op.execute("""DROP VIEW daily_summary""")

Listing 8-12New migration to add a view with raw SQL

```

我们现在可以创建一个表对象来引用这个视图，允许我们在 SQLAlchemy 中生成查询:

```py
daily_summary_view = Table(
    "daily_summary",
    metadata,
    sqlalchemy.Column("sensor_name", sqlalchemy.String),
    sqlalchemy.Column("data", JSONB),
    sqlalchemy.Column("count", sqlalchemy.Integer),
    info={"is_view": True},
)

```

info 行允许我们设置任意的元数据。在这种情况下，在`env.py`文件中使用`is_view`元数据来配置 alembic，以便在自动生成修订时忽略带有该标记的表。如果没有这些，alembic 将试图创建与我们的视图相冲突的匹配表。需要修改`env.py`文件以包含清单 [8-13](#PC48) 中给出的函数，并且两个`context.configure(...)`函数调用必须将`include_object=include_object`添加到参数中。

```py
from logging.config import fileConfig

from sqlalchemy import engine_from_config
from sqlalchemy import pool
from alembic import context

from apd.aggregation.database import metadata as target_metadata

def include_object(object, name, type_, reflected, compare_to):
    if object.info.get("is_view", False):
        return False
    return True

def run_migrations_online():
    connectable = engine_from_config(
        config.get_section(config.config_ini_section),
        prefix="sqlalchemy.",
        poolclass=pool.NullPool,
    )

    with connectable.connect() as connection:
        context.configure(
            connection=connection,
            target_metadata=target_metadata,
            include_object=include_object,
        )

        with context.begin_transaction():
            context.run_migrations()

Listing 8-13Changes to env.py to enable Table objects to represent views

```

通过前面的更改，在执行相同的 SQL 语句时，可以将汇总 SQL 语句简化为`db_session.query(daily_summary_view)`。每次使用视图时，都应该仔细考虑这种变化。在 SQL 语句上使用视图通常不会更清楚，但是对于更复杂的查询，我建议您记住这种未被充分利用的技术。

### 可供选择的事物

对于在异步上下文中与 SQL 数据库进行交互，我推荐部分使用 SQLAlchemy，但这还不够完美。根据您的使用情况，有一些替代方法可能是合适的。

还有一些 async-native ORM 正在开发中，比如*乌龟 ORM* 。它从根本上支持 asyncio，所以它不会遇到 SQLAlchemy 遇到的潜在阻塞问题。它目前是一个年轻的项目，所以虽然它是一个有趣的方法，我会继续关注它，但我现在不能推荐它用于生产代码。

另一种方法是使用类似于 *asyncpg* 的工具降低到较低的数据库集成级别。这允许与数据库进行完全异步的交互，而不需要将工作交给线程。缺点是没有内置的 SQL 生成器，所以它明显不太用户友好，并且您更容易出错。一些需要特别快的数据库连接的简单应用程序确实使用了这种方法，但是我不建议在一般情况下使用这种方法。

最后，对于 SQLAlchemy 导致查询阻塞的风险，有一种实用的方法，我在本章前面提到过。有时，最好的解决方案是接受风险，因为使用 SQLAlchemy 的好处自然会超过性能损失的后果。这在服务器端应用程序中是绝对不可接受的，在服务器端应用程序中，阻塞和减速会导致客户端性能严重下降，但是在客户端应用程序中，使用 asyncio 来提高代码的性能(否则代码将是单线程的),只使用 SQLAlchemy 并尽最大努力在执行器中运行阻塞代码几乎没有什么负面影响。

## 异步代码中的全局变量

尤其是在 web 开发中，经常会发现自己处于这样一种情况:你总是需要访问一个特定的对象，这意味着你所有的函数都需要将这个对象作为参数。这通常是请求对象，表示服务器当前正在处理的 HTTP 请求。还有一个配置对象也很常见，在我们的异步代码中，我们发现自己向许多函数签名添加了一个`ClientSession`对象，而不是为每个 HTTP 请求实例化一个新的对象。

所有这些都是全局变量的概念吸引人的地方。Django 和 Flask 都提供了访问配置的全局方式(`django.settings`和`flask.current_app.config`)，Flask 还通过`flask.request`提供请求。

你经常听到人们批评使用全局变量的代码，说这证明你的应用程序没有被正确设计。我采取了一种更务实的观点:几乎每个函数*潜在需要的对象不应该存在，但有时它们会存在。因此，它们应该是全局可用的，以防止它们污染整个系统的函数签名。*

让我们使用 Python 的`contextvars`特性使我们的`ClientSession`对象成为这些全局可用的项目之一。上下文变量是线程局部变量思想的发展:变量是全局范围的，但是对于不同的并发代码可以有不同的值。通过`threading.Local()`创建的线程局部变量允许通过属性访问来存储和检索任意数据，但只能在一个线程内。任何其他并发线程将看不到其他线程存储的数据；每个线程都可以有自己的变量值。

我们的代码不是线程化的；它使用异步函数调用来引入并发性，因此线程局部变量将总是向所有并发任务显示相同的数据。这就是上下文变量有用的地方；它们为任意范围的值提供相同的范围，而不是将范围限制为总是当前线程。

上下文变量是用`contextvars.ContextVar(...)`构造函数定义的，它将变量的名称作为参数。

```py
from contextvars import ContextVar
import aiohttp

http_session_var: ContextVar[aiohttp.ClientSession] = ContextVar("http_session")

```

`ContextVar`对象不直接存储值；它静默地委托给上下文对象。您可以手动实例化上下文对象，并使用该上下文执行一个函数，但是不需要使用异步代码来执行。 <sup>[13](#Fn13)</sup> 每当一个协程被调度为一个任务时，就会分配一个新的上下文，并从父任务的上下文中复制值。

可以使用`set(...)`方法为`ContextVar`设置值，并使用`get()`方法检索值。如果一段代码试图在当前上下文中没有设置的上下文变量上调用`get()`，就会引发 LookupError。必要的修改如表 [8-5](#Tab5) 所示。

表 8-5

get_data_points(...)所以 HTTP 客户端是作为上下文变量而不是参数传递的

<colgroup><col class="tcol1 align-left"> <col class="tcol2 align-left"></colgroup> 
| `http = http_session_var.get()``to_get = http.get(url, headers=headers)``async with to_get as request:``result = await request.json()``ok = request.status == 200` | `async with aiohttp.ClientSession() as http:``http_session_var.set(http)``tasks = [``get_data_points(server, api_key)``for server in servers``]` |

也可以使用`set(...)`的返回值临时覆盖上下文变量的值。这通常是不必要的，但是如果您确实需要在协程中更改一个变量，然后再将它改回来，那么这是首选模式:

```py
reset_token = http_session_var.set(mockclient)
try:
    datapoints = await get_data_points("http://localhost", "")
finally:
    http_session_var.reset(reset_token)

```

Exercise 8-1: Extending The API

本章介绍了许多新概念，并涉及一些复杂的测试设置。这段代码很复杂，但是我们需要有信心在新版本发布时更新它。

目前，除了传感器的 URL 之外，我们没有传感器的任何标识符，并且随着 IP 地址的重新分配，这种标识符会随着时间而改变。我们应该创建一种识别传感器端点的方法，这样我们就可以更容易地从单个传感器中找到数据。向提供新端点的 apd.sensors 包添加新的 v2.1 API。这个端点应该是

```py
@version.route("/deployment_id")
def deployment_id() -> t.Tuple[t.Dict[str, t.Any], int, t.Dict[str, str]]:
    headers = {"Content-Security-Policy": "default-src 'none'"}
    data = {"deployment_id": flask.current_app.config["APD_SENSORS_DEPLOYMENT_ID"]}
    return data, 200, headers

```

您将需要修改测试设置的许多部分来适应这种变化，包括以前 API 的 fixture 代码。记住，目的不是让旧 API 的测试代码永远不变，只是面向用户的 API 本身。

一旦完成了这些，更新 apd.aggregation 包以将`deployment_id`存储为`DataPoint`的属性，并使用 v2.1 API 从端点检索部署 ID。

这是一个显著的变化，相当于 apd.sensors 包的一个主要版本，也可能是本书中最困难的练习。然而，这是您迟早要在实际代码中进行的那种更改，所以练习一下是有好处的。

这两个更改的完整版本都在本章附带的代码中。

## 摘要

在本章中，我们已经讨论了运行异步代码的许多实际问题，尤其是在异步环境中使用数据库时可能会遇到的一些困难。要记住的最重要的事情是，无论是处理 SQLAlchemy、Django ORM，还是连接到另一个使用同步代码的数据库类型，run_in_executor 模式都是必要的，以避免显著降低性能的阻塞行为。但是，需要在性能优势和代码可读性优势之间取得平衡。这可能是您在编写异步代码时应该记住的最重要的平衡。

我们还讨论了许多在编写 Python 代码时通常有用的技术，无论是异步的还是其他的。使用`contextlib`的定制数据类和上下文管理器是非常有用的功能，您将在许多不同的上下文中使用它们。上下文变量和高效的 ORM 查询都非常有用，但程度较低。

在本章的过程中,`apd.aggregation`包已经成长了很多，达到了足以在生产中使用的质量。在下一章，我们将着眼于分析数据和构建有用的用户界面来显示报告。

### 额外资源

我推荐以下资源，以获取本章所涵盖主题的更多信息:

*   有关在 Django 的 ORM 中实现自定义 SQL 行为的信息，请参见 [`https://docs.djangoproject.com/en/3.0/ref/models/expressions/`](https://docs.djangoproject.com/en/3.0/ref/models/expressions/) 。

*   关于混合属性的完整 SQLAlchemy 文档，包括一些不常用特性的信息，位于 [`https://docs.sqlalchemy.org/en/14/orm/extensions/hybrid.html`](https://docs.sqlalchemy.org/en/14/orm/extensions/hybrid.html) 。

*   Django 关于混合同步和异步代码的文档在 [`https://docs.djangoproject.com/en/3.0/topics/async/`](https://docs.djangoproject.com/en/3.0/topics/async/) 中，其中包括数据库操作和帮助器函数的信息，用于在 Django 应用程序中弥补同步和异步代码之间的差距。

*   位于 [`https://explain.depesz.com/`](https://explain.depesz.com/) 的 web 应用程序是一个有用的工具，通过将 PostgreSQL EXPLAIN ANALYZE 语句重新格式化为表格并对计时信息进行颜色编码，可以帮助理解它们的结果。

*   [`https://github.com/getsentry/responses`](https://github.com/getsentry/responses) 是一个有用的库，用于在使用请求 HTTP 库时创建模拟 HTTP 响应。

<aside aria-label="Footnotes" class="FootnoteSection" epub:type="footnotes">Footnotes [1](#Fn1_source)

在启动线程之前，可以使用`thread_obj.daemon = True`将它标记为“守护”线程。这将允许进程在线程仍在运行的情况下结束，但这会导致线程在操作中途终止。通常最好使用 sentinel 值来允许所有线程干净地关闭。

  [2](#Fn2_source)

返回一个迭代器，依次包含每个迭代器中的项目。

  [3](#Fn3_source)

这是通过标准库中的`inspect.isgeneratorfunction(...)`函数完成的。

  [4](#Fn4_source)

package fixture 作用域目前是试验性的，可能会在 pytest 的未来版本中删除。我最常用的作用域依次是函数、会话、类和模块。我还没有理由使用包范围。

  [5](#Fn5_source)

如果您想亲自看到这一点，可以在 fixtures 中添加`print(...)`调用，并使用`-s`开关运行 pytest，以防止捕获 stdout。但是，请注意，pytest 不保证它决定运行测试的顺序，因此这对于调试问题比验证没有问题发生更有用。

  [6](#Fn6_source)

`response['body']`行不通。

  [7](#Fn7_source)

需要强调的是，这些测试功能是为了补充功能测试套件，而不是取代它。我们的测试运行不会更快，除非我们排除功能测试。

  [8](#Fn8_source)

有时甚至多次涉及同一个表的连接，这会导致特别混乱的代码。

  [9](#Fn9_source)

如果您的查询涉及大量参数，这可能会很棘手。sqlalchemy-utils 包中有一个名为 analyze 的函数将执行分析，但它也解析结果，而不是显示标准格式。下面的(相当复杂的)一行程序，当放在您的`.pdbrc`文件中时，将允许您从 pdb 提示符下运行解释分析查询:

```py
(Pdb) explain_analyze example_query db_session
GroupAggregate  (cost=25.61..25.63 rows=1 width=72) (actual time=0.022..0.022 rows=0 loops=1)
  Group Key: sensor_name, data
  ->  Sort  (cost=25.61..25.62 rows=1 width=68) (actual time=0.022..0.022 rows=0 loops=1)
        Sort Key: data
        Sort Method: quicksort  Memory: 25kB
        ->  Seq Scan on sensor_values  (cost=0.00..25.60 rows=1 width=68) (actual time=0.018..0.018 rows=0 loops=1)
              Filter: (((sensor_name)::text = 'ACStatus'::text) AND ((collected_at)::date = CURRENT_DATE))
Planning Time: 1.867 ms
Execution Time: 0.063 ms

```

```py
alias explain_analyze !_compiled=(%1).selectable.compile();_rows=(%2).execute("EXPLAIN ANALYZE "+ str(_compiled), params=_compiled.params); print("\n".join(str(_row[0]) for _row in _rows))

```

 and used as follows:

我已经将它包含在从本章开始的项目`.pdbrc`中，所以如果你跟随附带的代码，它将对你可用。

  [10](#Fn10_source)

这是常用的，但请不要这样做。不是每个人都有姓和名；没有一种通用的方法可以把一个全名分成几个部分，也没有办法把几个部分组合成一个全名。另见《程序员相信名字的假话》(及相关文章， *…时间*， *…地址*， *…地图*， *…性别*等。).作为工程师，我们有责任指出这些缺陷，就像我们在 20 世纪 90 年代指出两位数日期的缺陷一样。

  [11](#Fn11_source)

这些比较器只有在被 SQLAlchemy 查询时才起作用；它们不会改变数据库中唯一约束的行为。您将需要确保这些约束也是正确的，例如通过将它们指定为

```py
Index("unique_username_idx", func.lower(user_table.c.username), unique=True)

```

  [12](#Fn12_source)

除非 postgresql 的物化视图特性，该特性在显式刷新之前缓存其结果。

  [13](#Fn13_source)

对于同步代码，可以创建一个新的上下文，并调用一个使用该上下文的函数

```py
context = contextvars.copy_context()
context.run(your_callable)

```

 </aside>